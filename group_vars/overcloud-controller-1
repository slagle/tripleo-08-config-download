deploy_server_id: 083b33fa-9146-4fa6-8161-9fb5fdd0cc3f

NetworkDeployment:
  config: |
    #!/bin/bash
    # The following environment variables may be set to substitute in a
    # custom bridge or interface name.  Normally these are provided by the calling
    # SoftwareConfig resource, but they may also be set manually for testing.
    # $bridge_name : The bridge device name to apply
    # $interface_name : The interface name to apply
    #
    # Also this token is replaced via a str_replace in the SoftwareConfig running
    # the script - in future we may extend this to also work with a variable, e.g
    # a deployment input via input_values
    # {"network_config": [{"members": [{"name": "interface_name", "primary": true, "type": "interface"}], "name": "bridge_name", "type": "ovs_bridge", "use_dhcp": true}]} : the json serialized os-net-config config to apply
    #
    set -eux
    
    function get_metadata_ip() {
    
      local METADATA_IP
    
      # Look for a variety of Heat transports
      # FIXME: Heat should provide a way to obtain this in a single place
      for URL in os-collect-config.cfn.metadata_url os-collect-config.heat.auth_url os-collect-config.request.metadata_url os-collect-config.zaqar.auth_url; do
        METADATA_IP=$(os-apply-config --key $URL --key-default '' --type raw 2>/dev/null | sed -e 's|http.*://\([^:]*\).*|\1|')
        [ -n "$METADATA_IP" ] && break
      done
    
      echo $METADATA_IP
    
    }
    
    function is_local_ip() {
      local IP_TO_CHECK=$1
      if ip -o a | grep "inet6\? $IP_TO_CHECK/" &>/dev/null; then
        return 0
      else
        return 1
      fi
    }
    
    function ping_metadata_ip() {
      local METADATA_IP=$(get_metadata_ip)
    
      if [ -n "$METADATA_IP" ] && ! is_local_ip $METADATA_IP; then
    
        echo -n "Trying to ping metadata IP ${METADATA_IP}..."
    
        local COUNT=0
        until ping -c 1 $METADATA_IP &> /dev/null; do
          COUNT=$(( $COUNT + 1 ))
          if [ $COUNT -eq 10 ]; then
            echo "FAILURE"
            echo "$METADATA_IP is not pingable." >&2
            exit 1
          fi
        done
        echo "SUCCESS"
    
      else
        echo "No metadata IP found. Skipping."
      fi
    }
    
    function configure_safe_defaults() {
    
    [[ $? == 0 ]] && return 0
    
    cat > /etc/os-net-config/dhcp_all_interfaces.yaml <<EOF_CAT
    # This file is an autogenerated safe defaults file for os-net-config
    # which runs DHCP on all discovered interfaces to ensure connectivity
    # back to the undercloud for updates
    network_config:
    EOF_CAT
    
        for iface in $(ls /sys/class/net | grep -v -e ^lo$ -e ^vnet$); do
            local mac_addr_type="$(cat /sys/class/net/${iface}/addr_assign_type)"
            if [ "$mac_addr_type" != "0" ]; then
                echo "Device has generated MAC, skipping."
            else
                HAS_LINK="$(cat /sys/class/net/${iface}/carrier || echo 0)"
    
                TRIES=10
                while [ "$HAS_LINK" == "0" -a $TRIES -gt 0 ]; do
                    # Need to set the link up on each iteration
                    ip link set dev $iface up &>/dev/null
                    HAS_LINK="$(cat /sys/class/net/${iface}/carrier || echo 0)"
                    if [ "$HAS_LINK" == "1" ]; then
                        break
                    else
                        sleep 1
                    fi
                    TRIES=$(( TRIES - 1 ))
                done
                if [ "$HAS_LINK" == "1" ] ; then
    cat >> /etc/os-net-config/dhcp_all_interfaces.yaml <<EOF_CAT
      -
        type: interface
        name: $iface
        use_dhcp: true
    EOF_CAT
                fi
            fi
        done
        set +e
        os-net-config -c /etc/os-net-config/dhcp_all_interfaces.yaml -v --detailed-exit-codes --cleanup
        RETVAL=$?
        set -e
        if [[ $RETVAL == 2 ]]; then
            ping_metadata_ip
        elif [[ $RETVAL != 0 ]]; then
            echo "ERROR: configuration of safe defaults failed."
        fi
    }
    
    if [ -n '{"network_config": [{"members": [{"name": "interface_name", "primary": true, "type": "interface"}], "name": "bridge_name", "type": "ovs_bridge", "use_dhcp": true}]}' ]; then
        if [ -z "${disable_configure_safe_defaults:-}" ]; then
            trap configure_safe_defaults EXIT
        fi
    
        mkdir -p /etc/os-net-config
        # Note these variables come from the calling heat SoftwareConfig
        echo '{"network_config": [{"members": [{"name": "interface_name", "primary": true, "type": "interface"}], "name": "bridge_name", "type": "ovs_bridge", "use_dhcp": true}]}' > /etc/os-net-config/config.json
    
        if [ "$(type -t network_config_hook)" = "function" ]; then
            network_config_hook
        fi
    
        sed -i "s/bridge_name/${bridge_name:-''}/" /etc/os-net-config/config.json
        sed -i "s/interface_name/${interface_name:-''}/" /etc/os-net-config/config.json
    
        set +e
        os-net-config -c /etc/os-net-config/config.json -v --detailed-exit-codes
        RETVAL=$?
        set -e
    
        if [[ $RETVAL == 2 ]]; then
            ping_metadata_ip
    
            #NOTE: dprince this udev rule can apparently leak DHCP processes?
            # https://bugs.launchpad.net/tripleo/+bug/1538259
            # until we discover the root cause we can simply disable the
            # rule because networking has already been configured at this point
            if [ -f /etc/udev/rules.d/99-dhcp-all-interfaces.rules ]; then
                rm /etc/udev/rules.d/99-dhcp-all-interfaces.rules
            fi
    
        elif [[ $RETVAL != 0 ]]; then
            echo "ERROR: os-net-config configuration failed." >&2
            exit 1
        fi
    fi
  creation_time: "2018-03-09T19:45:29Z"
  deployment_name: NetworkDeployment
  group: script
  id: ddb3f432-d50f-4a67-bc66-f4302f688b67
  inputs:
    - name: interface_name
      description: None
      type: String
      value: |-
        nic1
    - name: bridge_name
      description: None
      type: String
      value: |-
        br-ex
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-Controller-xnl34i5ufw7g-1-lyve2t6wza3v-NetworkDeployment-mc42lcbgqkug-TripleOSoftwareDeployment-pfacpqj6pgyd/642ac434-14aa-48fb-a13d-d883f5e01013
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:

ControllerUpgradeInitDeployment:
  config: |
    #!/bin/bash
    
    if [[ -f /etc/resolv.conf.save ]] ; then rm /etc/resolv.conf.save; fi
    
  creation_time: "2018-03-09T19:45:48Z"
  deployment_name: ControllerUpgradeInitDeployment
  group: script
  id: f741a9bb-406f-4a53-a85b-7b2543707c03
  inputs:
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-Controller-xnl34i5ufw7g-1-lyve2t6wza3v-ControllerUpgradeInitDeployment-ji62h2mpfvae/88885914-ffcc-4c7a-848a-855f76930027
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:

UpdateDeployment:
  config: |
    #!/bin/bash
    
    set -eu
    
    DEBUG="true" # set false if the verbosity is a problem
    SCRIPT_NAME=$(basename $0)
    function log_debug {
      if [[ $DEBUG = "true" ]]; then
        echo "`date` $SCRIPT_NAME tripleo-upgrade $(facter hostname) $1"
      fi
    }
    
    function is_bootstrap_node {
      if [ "$(hiera -c /etc/puppet/hiera.yaml bootstrap_nodeid | tr '[:upper:]' '[:lower:]')" = "$(facter hostname | tr '[:upper:]' '[:lower:]')" ]; then
        log_debug "Node is bootstrap"
        echo "true"
      fi
    }
    
    function check_resource_pacemaker {
      if [ "$#" -ne 3 ]; then
        echo_error "ERROR: check_resource function expects 3 parameters, $# given"
        exit 1
      fi
    
      local service=$1
      local state=$2
      local timeout=$3
    
      if [[ -z $(is_bootstrap_node) ]] ; then
        log_debug "Node isn't bootstrap, skipping check for $service to be $state here "
        return
      else
        log_debug "Node is bootstrap checking $service to be $state here"
      fi
    
      if [ "$state" = "stopped" ]; then
        match_for_incomplete='Started'
      else # started
        match_for_incomplete='Stopped'
      fi
    
      nodes_local=$(pcs status  | grep ^Online | sed 's/.*\[ \(.*\) \]/\1/g' | sed 's/ /\|/g')
      if timeout -k 10 $timeout crm_resource --wait; then
        node_states=$(pcs status --full | grep "$service" | grep -v Clone | { egrep "$nodes_local" || true; } )
        if echo "$node_states" | grep -q "$match_for_incomplete"; then
          echo_error "ERROR: cluster finished transition but $service was not in $state state, exiting."
          exit 1
        else
          echo "$service has $state"
        fi
      else
        echo_error "ERROR: cluster remained unstable for more than $timeout seconds, exiting."
        exit 1
      fi
    
    }
    
    function pcmk_running {
      if [[ $(systemctl is-active pacemaker) = "active" ]] ; then
        echo "true"
      fi
    }
    
    function is_systemd_unknown {
      local service=$1
      if [[ $(systemctl is-active "$service") = "unknown" ]]; then
        log_debug "$service found to be unkown to systemd"
        echo "true"
      fi
    }
    
    function grep_is_cluster_controlled {
      local service=$1
      if [[ -n $(systemctl status $service -l | grep Drop-In -A 5 | grep pacemaker) ||
          -n $(systemctl status $service -l | grep "Cluster Controlled $service") ]] ; then
        log_debug "$service is pcmk managed from systemctl grep"
        echo "true"
      fi
    }
    
    
    function is_systemd_managed {
      local service=$1
      #if we have pcmk check to see if it is managed there
      if [[ -n $(pcmk_running) ]]; then
        if [[ -z $(pcs status --full | grep $service)  && -z $(is_systemd_unknown $service) ]] ; then
          log_debug "$service found to be systemd managed from pcs status"
          echo "true"
        fi
      else
        # if it is "unknown" to systemd, then it is pacemaker managed
        if [[  -n $(is_systemd_unknown $service) ]] ; then
          return
        elif [[ -z $(grep_is_cluster_controlled $service) ]] ; then
          echo "true"
        fi
      fi
    }
    
    function is_pacemaker_managed {
      local service=$1
      #if we have pcmk check to see if it is managed there
      if [[ -n $(pcmk_running) ]]; then
        if [[ -n $(pcs status --full | grep $service) ]]; then
          log_debug "$service found to be pcmk managed from pcs status"
          echo "true"
        fi
      else
        # if it is unknown to systemd, then it is pcmk managed
        if [[ -n $(is_systemd_unknown $service) ]]; then
          echo "true"
        elif [[ -n $(grep_is_cluster_controlled $service) ]] ; then
          echo "true"
        fi
      fi
    }
    
    function is_managed {
      local service=$1
      if [[ -n $(is_pacemaker_managed $service) || -n $(is_systemd_managed $service) ]]; then
        echo "true"
      fi
    }
    
    function check_resource_systemd {
    
      if [ "$#" -ne 3 ]; then
        echo_error "ERROR: check_resource function expects 3 parameters, $# given"
        exit 1
      fi
    
      local service=$1
      local state=$2
      local timeout=$3
      local check_interval=3
    
      if [ "$state" = "stopped" ]; then
        match_for_incomplete='active'
      else # started
        match_for_incomplete='inactive'
      fi
    
      log_debug "Going to check_resource_systemd for $service to be $state"
    
      #sanity check is systemd managed:
      if [[ -z $(is_systemd_managed $service) ]]; then
        echo "ERROR - $service not found to be systemd managed."
        exit 1
      fi
    
      tstart=$(date +%s)
      tend=$(( $tstart + $timeout ))
      while (( $(date +%s) < $tend )); do
        if [[ "$(systemctl is-active $service)" = $match_for_incomplete ]]; then
          echo "$service not yet $state, sleeping $check_interval seconds."
          sleep $check_interval
        else
          echo "$service is $state"
          return
        fi
      done
    
      echo "Timed out waiting for $service to go to $state after $timeout seconds"
      exit 1
    }
    
    
    function check_resource {
      local service=$1
      local pcmk_managed=$(is_pacemaker_managed $service)
      local systemd_managed=$(is_systemd_managed $service)
    
      if [[ -n $pcmk_managed && -n $systemd_managed ]] ; then
        log_debug "ERROR $service managed by both systemd and pcmk - SKIPPING"
        return
      fi
    
      if [[ -n $pcmk_managed ]]; then
        check_resource_pacemaker $@
        return
      elif [[ -n $systemd_managed ]]; then
        check_resource_systemd $@
        return
      fi
      log_debug "ERROR cannot check_resource for $service, not managed here?"
    }
    
    function manage_systemd_service {
      local action=$1
      local service=$2
      log_debug "Going to systemctl $action $service"
      systemctl $action $service
    }
    
    function manage_pacemaker_service {
      local action=$1
      local service=$2
      # not if pacemaker isn't running!
      if [[ -z $(pcmk_running) ]]; then
        echo "$(facter hostname) pacemaker not active, skipping $action $service here"
      elif [[ -n $(is_bootstrap_node) ]]; then
        log_debug "Going to pcs resource $action $service"
        pcs resource $action $service
      fi
    }
    
    function stop_or_disable_service {
      local service=$1
      local pcmk_managed=$(is_pacemaker_managed $service)
      local systemd_managed=$(is_systemd_managed $service)
    
      if [[ -n $pcmk_managed && -n $systemd_managed ]] ; then
        log_debug "Skipping stop_or_disable $service due to management conflict"
        return
      fi
    
      log_debug "Stopping or disabling $service"
      if [[ -n $pcmk_managed ]]; then
        manage_pacemaker_service disable $service
        return
      elif [[ -n $systemd_managed ]]; then
        manage_systemd_service stop $service
        return
      fi
      log_debug "ERROR: $service not managed here?"
    }
    
    function start_or_enable_service {
      local service=$1
      local pcmk_managed=$(is_pacemaker_managed $service)
      local systemd_managed=$(is_systemd_managed $service)
    
      if [[ -n $pcmk_managed && -n $systemd_managed ]] ; then
        log_debug "Skipping start_or_enable $service due to management conflict"
        return
      fi
    
      log_debug "Starting or enabling $service"
      if [[ -n $pcmk_managed ]]; then
        manage_pacemaker_service enable $service
        return
      elif [[ -n $systemd_managed ]]; then
        manage_systemd_service start $service
        return
      fi
      log_debug "ERROR $service not managed here?"
    }
    
    function restart_service {
      local service=$1
      local pcmk_managed=$(is_pacemaker_managed $service)
      local systemd_managed=$(is_systemd_managed $service)
    
      if [[ -n $pcmk_managed && -n $systemd_managed ]] ; then
        log_debug "ERROR $service managed by both systemd and pcmk - SKIPPING"
        return
      fi
    
      log_debug "Restarting $service"
      if [[ -n $pcmk_managed ]]; then
        manage_pacemaker_service restart $service
        return
      elif [[ -n $systemd_managed ]]; then
        manage_systemd_service restart $service
        return
      fi
      log_debug "ERROR $service not managed here?"
    }
    
    function echo_error {
        echo "$@" | tee /dev/fd2
    }
    
    # swift is a special case because it is/was never handled by pacemaker
    # when stand-alone swift is used, only swift-proxy is running on controllers
    function systemctl_swift {
        services=( openstack-swift-account-auditor openstack-swift-account-reaper openstack-swift-account-replicator openstack-swift-account \
                   openstack-swift-container-auditor openstack-swift-container-replicator openstack-swift-container-updater openstack-swift-container \
                   openstack-swift-object-auditor openstack-swift-object-replicator openstack-swift-object-updater openstack-swift-object openstack-swift-proxy )
        local action=$1
        case $action in
            stop)
                services=$(systemctl | grep openstack-swift- | grep running | awk '{print $1}')
                ;;
            start)
                enable_swift_storage=$(hiera -c /etc/puppet/hiera.yaml tripleo::profile::base::swift::storage::enable_swift_storage)
                if [[ $enable_swift_storage != "true" ]]; then
                    services=( openstack-swift-proxy )
                fi
                ;;
            *)  echo "Unknown action $action passed to systemctl_swift"
                exit 1
                ;; # shouldn't ever happen...
        esac
        for service in ${services[@]}; do
            manage_systemd_service $action $service
        done
    }
    
    # Special-case OVS for https://bugs.launchpad.net/tripleo/+bug/1635205
    # Update condition and add --notriggerun for +bug/1669714
    function special_case_ovs_upgrade_if_needed {
        if rpm -qa | grep "^openvswitch-2.5.0-14" || rpm -q --scripts openvswitch | awk '/postuninstall/,/*/' | grep "systemctl.*try-restart" ; then
            echo "Manual upgrade of openvswitch - ovs-2.5.0-14 or restart in postun detected"
            rm -rf OVS_UPGRADE
            mkdir OVS_UPGRADE && pushd OVS_UPGRADE
            echo "Attempting to downloading latest openvswitch with yumdownloader"
            yumdownloader --resolve openvswitch
            for pkg in $(ls -1 *.rpm);  do
                if rpm -U --test $pkg 2>&1 | grep "already installed" ; then
                    echo "Looks like newer version of $pkg is already installed, skipping"
                else
                    echo "Updating $pkg with --nopostun --notriggerun"
                    rpm -U --replacepkgs --nopostun --notriggerun $pkg
                fi
            done
            popd
        else
            echo "Skipping manual upgrade of openvswitch - no restart in postun detected"
        fi
    
    }
    
    # This code is meant to fix https://bugs.launchpad.net/tripleo/+bug/1686357 on
    # existing setups via a minor update workflow and be idempotent. We need to
    # run this before the yum update because we fix this up even when there are no
    # packages to update on the system (in which case the script exits).
    # This code must be called with set +eu (due to the ocf scripts being sourced)
    function fixup_wrong_ipv6_vip {
        # This XPath query identifies of all the VIPs in pacemaker with netmask /64. Those are IPv6 only resources that have the wrong netmask
        # This gives the address of the resource in the CIB, one address per line. For example:
        # /cib/configuration/resources/primitive[@id='ip-2001.db8.ca2.4..10']/instance_attributes[@id='ip-2001.db8.ca2.4..10-instance_attributes']\
        # /nvpair[@id='ip-2001.db8.ca2.4..10-instance_attributes-cidr_netmask']
        vip_xpath_query="//resources/primitive[@type='IPaddr2']/instance_attributes/nvpair[@name='cidr_netmask' and @value='64']"
        vip_xpath_xml_addresses=$(cibadmin --query --xpath "$vip_xpath_query" -e 2>/dev/null)
        # The following extracts the @id value of the resource
        vip_resources_to_fix=$(echo -e "$vip_xpath_xml_addresses" | sed -n "s/.*primitive\[@id='\([^']*\)'.*/\1/p")
        # Runnning this in a subshell so that sourcing files cannot possibly affect the running script
        (
            OCF_PATH="/usr/lib/ocf/lib/heartbeat"
            if [ -n "$vip_resources_to_fix" -a -f $OCF_PATH/ocf-shellfuncs -a -f $OCF_PATH/findif.sh ]; then
                source $OCF_PATH/ocf-shellfuncs
                source $OCF_PATH/findif.sh
                for resource in $vip_resources_to_fix; do
                    echo "Updating IPv6 VIP $resource with a /128 and a correct addrlabel"
                    # The following will give us something like:
                    # <nvpair id="ip-2001.db8.ca2.4..10-instance_attributes-ip" name="ip" value="2001:db8:ca2:4::10"/>
                    ip_cib_nvpair=$(cibadmin --query --xpath "//resources/primitive[@type='IPaddr2' and @id='$resource']/instance_attributes/nvpair[@name='ip']")
                    # Let's filter out the value of the nvpair to get the ip address
                    ip_address=$(echo $ip_cib_nvpair | xmllint --xpath 'string(//nvpair/@value)' -)
                    OCF_RESKEY_cidr_netmask="64"
                    OCF_RESKEY_ip="$ip_address"
                    # Unfortunately due to https://bugzilla.redhat.com/show_bug.cgi?id=1445628
                    # we need to find out the appropiate nic given the ip address.
                    nic=$(findif $ip_address | awk '{ print $1 }')
                    ret=$?
                    if [ -z "$nic" -o $ret -ne 0 ]; then
                        echo "NIC autodetection failed for VIP $ip_address, not updating VIPs"
                        # Only exits the subshell
                        exit 1
                    fi
                    ocf_run -info pcs resource update --wait "$resource" ip="$ip_address" cidr_netmask=128 nic="$nic" lvs_ipv6_addrlabel=true lvs_ipv6_addrlabel_value=99
                    ret=$?
                    if [ $ret -ne 0 ]; then
                        echo "pcs resource update for VIP $resource failed, not updating VIPs"
                        # Only exits the subshell
                        exit 1
                    fi
                done
            fi
        )
    }
    
    # https://bugs.launchpad.net/tripleo/+bug/1704131 guard against yum update
    # waiting for an existing process until the heat stack time out
    function check_for_yum_lock {
        if [[ -f /var/run/yum.pid ]] ; then
            ERR="ERROR existing yum.pid detected - can't continue! Please ensure
    there is no other package update process for the duration of the minor update
    worfklow. Exiting."
            echo $ERR
            exit 1
       fi
    }
    
    # This function tries to resolve an RPM dependency issue that can arise when
    # updating ceph packages on nodes that do not run the ceph-osd service. These
    # nodes do not require the ceph-osd package, and updates will fail if the
    # ceph-osd package cannot be updated because it's not available in any enabled
    # repo. The dependency issue is resolved by removing the ceph-osd package from
    # nodes that don't require it.
    #
    # No change is made to nodes that use the ceph-osd service (e.g. ceph storage
    # nodes, and hyperconverged nodes running ceph-osd and compute services). The
    # ceph-osd package is left in place, and the currently enabled repos will be
    # used to update all ceph packages.
    function yum_pre_update {
        echo "Checking for ceph-osd dependency issues"
    
        # No need to proceed if the ceph-osd package isn't installed
        if ! rpm -q ceph-osd >/dev/null 2>&1; then
            echo "ceph-osd package is not installed"
            return
        fi
    
        # Do not proceed if there's any sign that the ceph-osd package is in use:
        # - Are there OSD entries in /var/lib/ceph/osd?
        # - Are any ceph-osd processes running?
        # - Are there any ceph data disks (as identified by 'ceph-disk')
        if [ -n "$(ls -A /var/lib/ceph/osd 2>/dev/null)" ]; then
            echo "ceph-osd package is required (there are OSD entries in /var/lib/ceph/osd)"
            return
        fi
    
        if [ "$(pgrep -xc ceph-osd)" != "0" ]; then
            echo "ceph-osd package is required (there are ceph-osd processes running)"
            return
        fi
    
        if ceph-disk list |& grep -q "ceph data"; then
            echo "ceph-osd package is required (ceph data disks detected)"
            return
        fi
    
        # Get a list of all ceph packages available from the currently enabled
        # repos. Use "--showduplicates" to ensure the list includes installed
        # packages that happen to be up to date.
        local ceph_pkgs="$(yum list available --showduplicates 'ceph-*' |& awk '/^ceph/ {print $1}' | sort -u)"
    
        # No need to proceed if no ceph packages are available from the currently
        # enabled repos.
        if [ -z "$ceph_pkgs" ]; then
            echo "ceph packages are not available from any enabled repo"
            return
        fi
    
        # No need to proceed if the ceph-osd package *is* available
        if [[ $ceph_pkgs =~ ceph-osd ]]; then
            echo "ceph-osd package is available from an enabled repo"
            return
        fi
    
        echo "ceph-osd package is not required, but is preventing updates to other ceph packages"
        echo "Removing ceph-osd package to allow updates to other ceph packages"
        yum -y remove ceph-osd
    }
    #!/bin/bash
    
    # A heat-config-script which runs yum update during a stack-update.
    # Inputs:
    #   deploy_action - yum will only be run if this is UPDATE
    #   update_identifier - yum will only run for previously unused values of update_identifier
    #   command - yum sub-command to run, defaults to "update"
    #   command_arguments - yum command arguments, defaults to ""
    
    echo "Started yum_update.sh on server $deploy_server_id at `date`"
    echo -n "false" > $heat_outputs_path.update_managed_packages
    
    if [ -f /.dockerenv ]; then
        echo "Not running due to running inside a container"
        exit 0
    fi
    
    if [[ -z "$update_identifier" ]]; then
        echo "Not running due to unset update_identifier"
        exit 0
    fi
    
    timestamp_dir=/var/lib/overcloud-yum-update
    mkdir -p $timestamp_dir
    
    # sanitise to remove unusual characters
    update_identifier=${update_identifier//[^a-zA-Z0-9-_]/}
    
    # seconds to wait for this node to rejoin the cluster after update
    cluster_start_timeout=600
    galera_sync_timeout=1800
    cluster_settle_timeout=1800
    
    timestamp_file="$timestamp_dir/$update_identifier"
    if [[ -a "$timestamp_file" ]]; then
        echo "Not running for already-run timestamp \"$update_identifier\""
        exit 0
    fi
    touch "$timestamp_file"
    
    pacemaker_status=""
    # We include word boundaries in order to not match pacemaker_remote
    if hiera -c /etc/puppet/hiera.yaml service_names | grep -q '\bpacemaker\b'; then
        pacemaker_status=$(systemctl is-active pacemaker)
    fi
    
    # (NB: when backporting this s/pacemaker_short_bootstrap_node_name/bootstrap_nodeid)
    # This runs before the yum_update so we are guaranteed to run it even in the absence
    # of packages to update (the check for -z "$update_identifier" guarantees that this
    # is run only on overcloud stack update -i)
    if [[ "$pacemaker_status" == "active" && \
            "$(hiera -c /etc/puppet/hiera.yaml pacemaker_short_bootstrap_node_name | tr '[:upper:]' '[:lower:]')" == "$(facter hostname | tr '[:upper:]' '[:lower:]')" ]] ; then \
        # OCF scripts don't cope with -eu
        echo "Verifying if we need to fix up any IPv6 VIPs"
        set +eu
        fixup_wrong_ipv6_vip
        ret=$?
        set -eu
        if [ $ret -ne 0 ]; then
            echo "Fixing up IPv6 VIPs failed. Stopping here. (See https://bugs.launchpad.net/tripleo/+bug/1686357 for more info)"
            exit 1
        fi
    fi
    
    command_arguments=${command_arguments:-}
    
    # Always ensure yum has full cache
    check_for_yum_lock
    yum makecache || echo "Yum makecache failed. This can cause failure later on."
    
    # yum check-update exits 100 if updates are available
    check_for_yum_lock
    set +e
    check_update=$(yum check-update 2>&1)
    check_update_exit=$?
    set -e
    
    if [[ "$check_update_exit" == "1" ]]; then
        echo "Failed to check for package updates"
        echo "$check_update"
        exit 1
    elif [[ "$check_update_exit" != "100" ]]; then
        echo "No packages require updating"
        exit 0
    fi
    
    # special case https://bugs.launchpad.net/tripleo/+bug/1635205 +bug/1669714
    special_case_ovs_upgrade_if_needed
    
    # Resolve any RPM dependency issues before attempting the update
    check_for_yum_lock
    yum_pre_update
    
    if [[ "$pacemaker_status" == "active" ]] ; then
        echo "Pacemaker running, stopping cluster node and doing full package update"
        node_count=$(pcs status xml | grep -o "<nodes_configured.*/>" | grep -o 'number="[0-9]*"' | grep -o "[0-9]*")
        if [[ "$node_count" == "1" ]] ; then
            echo "Active node count is 1, stopping node with --force"
            pcs cluster stop --force
        else
            pcs cluster stop
        fi
    else
        echo "Upgrading Puppet modules and dependencies"
        check_for_yum_lock
        yum -q -y update puppet-tripleo
        yum deplist puppet-tripleo | awk '/dependency/{print $2}' | xargs yum -q -y update
        echo "Upgrading other packages is handled by config management tooling"
        echo -n "true" > $heat_outputs_path.update_managed_packages
        exit 0
    fi
    
    command=${command:-update}
    full_command="yum -q -y $command $command_arguments"
    
    echo "Running: $full_command"
    check_for_yum_lock
    result=$($full_command)
    return_code=$?
    echo "$result"
    echo "yum return code: $return_code"
    
    if [[ "$pacemaker_status" == "active" ]] ; then
        echo "Starting cluster node"
        pcs cluster start
    
        hostname=$(hostname -s)
        tstart=$(date +%s)
        while [[ "$(pcs status | grep "^Online" | grep -F -o $hostname)" == "" ]]; do
            sleep 5
            tnow=$(date +%s)
            if (( tnow-tstart > cluster_start_timeout )) ; then
                echo "ERROR $hostname failed to join cluster in $cluster_start_timeout seconds"
                pcs status
                exit 1
            fi
        done
    
        RETVAL=$( pcs resource show galera-master | grep wsrep_cluster_address | grep -q `crm_node -n` ; echo $? )
    
        if [[ $RETVAL -eq 0 && -e /etc/sysconfig/clustercheck ]]; then
            tstart=$(date +%s)
            while ! clustercheck; do
                sleep 5
                tnow=$(date +%s)
                if (( tnow-tstart > galera_sync_timeout )) ; then
                    echo "ERROR galera sync timed out"
                    exit 1
                fi
            done
        fi
    
        echo "Waiting for pacemaker cluster to settle"
        if ! timeout -k 10 $cluster_settle_timeout crm_resource --wait; then
            echo "ERROR timed out while waiting for the cluster to settle"
            exit 1
        fi
    
        pcs status
    fi
    
    
    echo "Finished yum_update.sh on server $deploy_server_id at `date` with return code: $return_code"
    
    exit $return_code
  creation_time: "2018-03-09T19:45:48Z"
  deployment_name: UpdateDeployment
  group: script
  id: 3ecec6b6-b266-4a61-8cb3-9065dd3e74a3
  inputs:
    - name: update_identifier
      description: yum will only run for previously unused values of update_identifier
      type: String
      value: |-
        
    - name: command
      description: yum sub-command to run, defaults to "update"
      type: String
      value: |-
        update
    - name: command_arguments
      description: yum command arguments, defaults to ""
      type: String
      value: |-
        
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-Controller-xnl34i5ufw7g-1-lyve2t6wza3v-UpdateDeployment-ovjauq2muapp/7dea53d2-8c8e-4394-8b55-7c2cf21c3f90
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:
    - name: update_managed_packages
      description: boolean value indicating whether to upgrade managed packages
      type: String

ControllerDeployment:
  config:
    {
      "hierarchy": [
        "\"%{::uuid}\"", 
        "heat_config_%{::deploy_config_name}", 
        "config_step", 
        "controller_extraconfig", 
        "extraconfig", 
        "service_names", 
        "service_configs", 
        "controller", 
        "bootstrap_node", 
        "all_nodes", 
        "vip_data", 
        "net_ip_map", 
        "\"%{::osfamily}\"", 
        "neutron_bigswitch_data", 
        "neutron_cisco_data", 
        "cisco_n1kv_data", 
        "midonet_data", 
        "cisco_aci_data"
      ], 
      "datafiles": {
        "net_ip_map": {
          "tenant": "192.168.24.12", 
          "management": "192.168.24.12", 
          "tenant_uri": "192.168.24.12", 
          "ctlplane_uri": "192.168.24.12", 
          "management_uri": "192.168.24.12", 
          "management_subnet": "192.168.24.12/24", 
          "storage": "192.168.24.12", 
          "internal_api_subnet": "192.168.24.12/24", 
          "storage_subnet": "192.168.24.12/24", 
          "external_subnet": "192.168.24.12/24", 
          "ctlplane": "192.168.24.12", 
          "storage_mgmt_subnet": "192.168.24.12/24", 
          "external": "192.168.24.12", 
          "ctlplane_subnet": "192.168.24.12/24", 
          "storage_mgmt": "192.168.24.12", 
          "internal_api_uri": "192.168.24.12", 
          "external_uri": "192.168.24.12", 
          "storage_uri": "192.168.24.12", 
          "internal_api": "192.168.24.12", 
          "storage_mgmt_uri": "192.168.24.12", 
          "tenant_subnet": "192.168.24.12/24"
        }, 
        "extraconfig": {}, 
        "service_configs": {
          "cinder::db::mysql::host": "192.168.24.7", 
          "nova::rabbit_port": 5672, 
          "nova::wsgi::apache_placement::bind_host": "192.168.24.12", 
          "aodh::db::mysql::dbname": "aodh", 
          "heat::enable_proxy_headers_parsing": true, 
          "nova::api::sync_db_api": true, 
          "horizon::django_session_engine": "django.contrib.sessions.backends.cache", 
          "panko::keystone::authtoken::password": "aCVWZBffGWdcKuYpGjK6JN8YB", 
          "tripleo::profile::base::gnocchi::api::incoming_storage_driver": "redis", 
          "horizon::secret_key": "NmAmpduVXC", 
          "aodh::keystone::auth::public_url": "http://192.168.24.7:8042", 
          "snmp::snmpd_options": "-LS0-5d", 
          "heat::keystone::auth::region": "regionOne", 
          "ceilometer::keystone::authtoken::user_domain_name": "Default", 
          "tripleo::profile::pacemaker::cinder::volume_bundle::cinder_volume_docker_image": "docker.io/tripleomaster/centos-binary-cinder-volume:pcmklatest", 
          "tripleo::profile::base::glance::api::glance_nfs_enabled": false, 
          "tripleo::profile::pacemaker::cinder::volume_bundle::docker_volumes": [
            "/etc/hosts:/etc/hosts:ro", 
            "/etc/localtime:/etc/localtime:ro", 
            "/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro", 
            "/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro", 
            "/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro", 
            "/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro", 
            "/dev/log:/dev/log", 
            "/etc/ssh/ssh_known_hosts:/etc/ssh/ssh_known_hosts:ro", 
            "/etc/puppet:/etc/puppet:ro", 
            "/var/lib/kolla/config_files/cinder_volume.json:/var/lib/kolla/config_files/config.json:ro", 
            "/var/lib/config-data/puppet-generated/cinder/:/var/lib/kolla/config_files/src:ro", 
            "/etc/iscsi:/var/lib/kolla/config_files/src-iscsid:ro", 
            "/etc/ceph:/var/lib/kolla/config_files/src-ceph:ro", 
            "/lib/modules:/lib/modules:ro", 
            "/dev/:/dev/", 
            "/run/:/run/", 
            "/sys:/sys", 
            "/var/lib/cinder:/var/lib/cinder", 
            "/var/log/containers/cinder:/var/log/cinder"
          ], 
          "memcached::max_memory": "50%", 
          "nova::db::mysql::host": "192.168.24.7", 
          "cinder::cron::db_purge::user": "cinder", 
          "aodh::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "tripleo.gnocchi_api.firewall_rules": {
            "129 gnocchi-api": {
              "dport": [
                8041, 
                13041
              ]
            }
          }, 
          "nova::policy::policies": {}, 
          "swift::keystone::auth::internal_url": "http://192.168.24.7:8080/v1/AUTH_%(tenant_id)s", 
          "tripleo::haproxy::crl_file": null, 
          "rabbitmq::default_user": "guest", 
          "horizon::cache_backend": "django.core.cache.backends.memcached.MemcachedCache", 
          "tripleo::profile::base::cinder::volume::rbd::cinder_rbd_user_name": "openstack", 
          "ceilometer::agent::auth::auth_project_domain_name": "Default", 
          "tripleo::profile::base::cinder::volume::iscsi::cinder_iscsi_helper": "lioadm", 
          "swift::keystone::auth::password": "hw46VkJ6ZuYX88bGnPYm7gwqt", 
          "gnocchi::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "nova::wsgi::apache_api::servername": "%{hiera('fqdn_internal_api')}", 
          "cinder::cron::db_purge::hour": "0", 
          "gnocchi::db::mysql::password": "R6Yn6NmnpN7nAfGMJfmmcsCRZ", 
          "rabbit_ipv6": false, 
          "glance::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "ceilometer::snmpd_readonly_user_password": "cda3740a255af021a3b68fd8bd89f70a2aeb80e1", 
          "ceilometer::debug": false, 
          "redis::sentinel_auth_pass": "DRMmzyDvmMuhG2AytUCHfpKTW", 
          "keystone::wsgi::apache::threads": 1, 
          "heat::keystone::auth_cfn::tenant": "service", 
          "cinder::wsgi::apache::ssl": false, 
          "heat::cron::purge_deleted::minute": "1", 
          "horizon::listen_ssl": false, 
          "cinder::volume::manage_service": false, 
          "nova::rabbit_password": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "neutron::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "gnocchi::storage::swift::swift_auth_version": 3, 
          "keystone::rabbit_use_ssl": "False", 
          "aodh::auth::auth_url": "http://192.168.24.7:5000", 
          "neutron::agents::dhcp::interface_driver": "neutron.agent.linux.interface.OVSInterfaceDriver", 
          "neutron::agents::metadata::metadata_host": "%{hiera('cloud_name_internal_api')}", 
          "tripleo.neutron_dhcp.firewall_rules": {
            "115 neutron dhcp input": {
              "dport": 67, 
              "proto": "udp"
            }, 
            "116 neutron dhcp output": {
              "dport": 68, 
              "chain": "OUTPUT", 
              "proto": "udp"
            }
          }, 
          "nova::placement::project_name": "service", 
          "keystone::enable_credential_setup": true, 
          "nova::placement_database_connection": "mysql+pymysql://nova_placement:ePtbUHubCqvanb7c8da8AAupz@192.168.24.7/nova_placement?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "keystone::endpoint::version": "", 
          "neutron::service_plugins": [
            "router", 
            "qos", 
            "trunk"
          ], 
          "gnocchi::keystone::auth::tenant": "service", 
          "panko::auth::auth_tenant_name": "service", 
          "snmpd_network": "192.168.24.12/24", 
          "keystone::enable_fernet_setup": true, 
          "redis_ipv6": false, 
          "glance::keystone::authtoken::project_domain_name": "Default", 
          "gnocchi::db::mysql::dbname": "gnocchi", 
          "tripleo.gnocchi_statsd.firewall_rules": {
            "140 gnocchi-statsd": {
              "dport": 8125, 
              "proto": "udp"
            }
          }, 
          "heat::api_cfn::bind_host": "192.168.24.12", 
          "keystone_ssl_certificate": "", 
          "heat::api::bind_host": "192.168.24.12", 
          "ceilometer::telemetry_secret": "aw3hrtZzDVBFz99nR2uJJ6rEY", 
          "keystone::cron::token_flush::minute": [
            "1"
          ], 
          "tripleo::profile::pacemaker::haproxy_bundle::internal_certs_directory": "/etc/pki/tls/certs/haproxy", 
          "aodh::keystone::auth::tenant": "service", 
          "tripleo::profile::base::cinder::volume::nfs::cinder_nfs_servers": [], 
          "heat::keystone::auth_cfn::public_url": "http://192.168.24.7:8000/v1", 
          "aodh::keystone::auth::admin_url": "http://192.168.24.7:8042", 
          "keystone::credential_keys": {
            "/etc/keystone/credential-keys/1": {
              "content": "5R4LI3h2Oi9NGG_jlfbLgKw8kz6oAdGkNvHAOeLXyU4="
            }, 
            "/etc/keystone/credential-keys/0": {
              "content": "lmFtpXxpYWOXnXjUyDT7s52NLxE6uiFqXcU7b8Ju7cE="
            }
          }, 
          "mysql::server::manage_config_file": true, 
          "swift::keystone::auth::configure_s3_endpoint": false, 
          "heat::wsgi::apache_api::servername": "%{hiera('fqdn_internal_api')}", 
          "redis::sentinel::redis_host": "%{hiera('bootstrap_nodeid_ip')}", 
          "redis::service_manage": false, 
          "cinder::rabbit_heartbeat_timeout_threshold": 60, 
          "neutron::plugins::ml2::firewall_driver": "iptables_hybrid", 
          "tripleo::profile::pacemaker::database::redis_bundle::redis_docker_image": "docker.io/tripleomaster/centos-binary-redis:pcmklatest", 
          "aodh::wsgi::apache::ssl": false, 
          "heat::keystone::auth::admin_url": "http://192.168.24.7:8004/v1/%(tenant_id)s", 
          "tripleo::haproxy::haproxy_service_manage": false, 
          "panko::expirer::hour": "0", 
          "tripleo::profile::base::haproxy::certificates_specs": {}, 
          "heat::wsgi::apache_api_cfn::servername": "%{hiera('fqdn_internal_api')}", 
          "keystone::wsgi::apache::servername_admin": "%{hiera('fqdn_ctlplane')}", 
          "tripleo::profile::base::swift::proxy::tls_proxy_port": "8080", 
          "neutron::db::sync::db_sync_timeout": 300, 
          "horizon::enforce_password_check": true, 
          "corosync_ipv6": false, 
          "keystone::enable_ssl": false, 
          "heat::cron::purge_deleted::hour": "0", 
          "keystone::rabbit_password": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "rabbitmq::ssl_depth": 1, 
          "gnocchi::keystone::authtoken::password": "R6Yn6NmnpN7nAfGMJfmmcsCRZ", 
          "nova::network::neutron::neutron_project_name": "service", 
          "tripleo::profile::base::sshd::bannertext": "", 
          "heat::keystone::auth_cfn::password": "HBqZMnZMW7ZhRJhEh3kqBT6BT", 
          "panko::policy::policies": {}, 
          "keystone::wsgi::apache::servername": "%{hiera('fqdn_internal_api')}", 
          "cinder::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "aodh::auth::auth_tenant_name": "service", 
          "nova::network::neutron::dhcp_domain": "", 
          "tripleo.cinder_volume.firewall_rules": {
            "120 iscsi initiator": {
              "dport": 3260
            }
          }, 
          "tripleo::profile::base::glance::api::tls_proxy_port": "9292", 
          "aodh::keystone::auth::region": "regionOne", 
          "nova::cell0_database_connection": "mysql+pymysql://nova:ePtbUHubCqvanb7c8da8AAupz@192.168.24.7/nova_cell0?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "neutron::plugins::ml2::network_vlan_ranges": [
            "datacentre:1:1000"
          ], 
          "tripleo::profile::base::snmp::snmpd_password": "cda3740a255af021a3b68fd8bd89f70a2aeb80e1", 
          "neutron::agents::metadata::metadata_ip": "%{hiera('nova_metadata_vip')}", 
          "neutron::dns_domain": "openstacklocal", 
          "tripleo::profile::base::snmp::snmpd_user": "ro_snmp_user", 
          "nova::scheduler::discover_hosts_in_cells_interval": -1, 
          "ceilometer::notification_driver": "messagingv2", 
          "tripleo::profile::base::cinder::volume::rbd::cinder_rbd_ceph_conf": "/etc/ceph/ceph.conf", 
          "tripleo::profile::base::docker::configure_network": true, 
          "cinder::scheduler::scheduler_driver": "cinder.scheduler.filter_scheduler.FilterScheduler", 
          "vswitch::ovs::enable_hw_offload": false, 
          "cinder::keystone::auth::public_url": "http://192.168.24.7:8776/v1/%(tenant_id)s", 
          "tripleo::profile::base::cinder::volume::rbd::cinder_rbd_pool_name": "volumes", 
          "tripleo::profile::pacemaker::database::mysql_bundle::bind_address": "%{hiera('fqdn_internal_api')}", 
          "memcached::listen_ip": "192.168.24.12", 
          "ceilometer::dispatcher::gnocchi::archive_policy": "low", 
          "cinder::api::nova_catalog_info": "compute:nova:internalURL", 
          "heat::notification_driver": "messagingv2", 
          "swift::proxy::authtoken::project_name": "service", 
          "heat::db::mysql::user": "heat", 
          "mysql_ipv6": false, 
          "aodh::keystone::authtoken::project_domain_name": "Default", 
          "glance::api::show_multiple_locations": false, 
          "tripleo::profile::base::swift::ringbuilder::swift_ring_get_tempurl": "http://192.168.24.1:8080/v1/AUTH_717af5f776724399988f7a1f32c8c7a5/overcloud-swift-rings/swift-rings.tar.gz?temp_url_sig=f46ff20cdd5195a8773b4e94636888b3ce51c1e4&temp_url_expires=1520709582", 
          "cinder::db::mysql::password": "ktePY6hcQhgyxk4KgZjgz78by", 
          "panko::keystone::auth::tenant": "service", 
          "aodh::keystone::authtoken::project_name": "service", 
          "neutron::server::allow_automatic_l3agent_failover": "True", 
          "panko::db::mysql::user": "panko", 
          "gnocchi::debug": false, 
          "gnocchi::wsgi::apache::bind_host": "192.168.24.12", 
          "neutron::agents::l3::agent_mode": "legacy", 
          "heat::keystone::authtoken::password": "HBqZMnZMW7ZhRJhEh3kqBT6BT", 
          "glance::keystone::auth::admin_url": "http://192.168.24.7:9292", 
          "nova::rabbit_use_ssl": "False", 
          "glance::backend::swift::swift_store_key": "wEWkucKnWXgaXbW3K73Z3s2Pa", 
          "heat::cron::purge_deleted::month": "*", 
          "glance_backend": "file", 
          "tripleo::profile::pacemaker::database::mysql_bundle::mysql_docker_image": "docker.io/tripleomaster/centos-binary-mariadb:pcmklatest", 
          "keystone::cron::token_flush::weekday": [
            "*"
          ], 
          "tripleo.aodh_api.firewall_rules": {
            "128 aodh-api": {
              "dport": [
                8042, 
                13042
              ]
            }
          }, 
          "heat::engine::max_resources_per_stack": 1000, 
          "gnocchi::metricd::metric_processing_delay": 30, 
          "neutron::quota::quota_port": "500", 
          "keystone::cron::token_flush::ensure": "present", 
          "heat::cron::purge_deleted::age": "30", 
          "aodh::api::host": "%{hiera('fqdn_internal_api')}", 
          "heat::engine::max_nested_stack_depth": 6, 
          "tripleo::profile::base::database::mysql::client::enable_ssl": false, 
          "panko::db::mysql::host": "192.168.24.7", 
          "swift::keystone::auth::region": "regionOne", 
          "enable_load_balancer": true, 
          "cinder::keystone::authtoken::project_name": "service", 
          "neutron::server::notifications::project_name": "service", 
          "heat::engine::heat_metadata_server_url": "http://192.168.24.7:8000", 
          "keystone::config::keystone_config": {
            "ec2/driver": {
              "value": "keystone.contrib.ec2.backends.sql.Ec2"
            }
          }, 
          "nova::keystone::authtoken::project_domain_name": "Default", 
          "tripleo::profile::base::cinder::volume::iscsi::cinder_iscsi_protocol": "iscsi", 
          "neutron::server::sync_db": true, 
          "redis::requirepass": "DRMmzyDvmMuhG2AytUCHfpKTW", 
          "ceilometer::agent::notification::manage_event_pipeline": true, 
          "cinder::volume::enabled": false, 
          "tripleo::profile::pacemaker::haproxy_bundle::haproxy_docker_image": "docker.io/tripleomaster/centos-binary-haproxy:pcmklatest", 
          "keystone::roles::admin::password": "CTU9zEJAv2ncR2nujhbvZJwsp", 
          "tripleo::haproxy::mysql_clustercheck": true, 
          "glance::db::mysql::dbname": "glance", 
          "rabbitmq::ssl_interface": "192.168.24.12", 
          "ceilometer::keystone::authtoken::project_domain_name": "Default", 
          "swift::proxy::keystone::operator_roles": [
            "admin", 
            "swiftoperator", 
            "ResellerAdmin"
          ], 
          "heat::rabbit_userid": "guest", 
          "neutron::keystone::authtoken::user_domain_name": "Default", 
          "tripleo.ntp.firewall_rules": {
            "105 ntp": {
              "dport": 123, 
              "proto": "udp"
            }
          }, 
          "tripleo::profile::base::keystone::heat_admin_domain": "heat_stack", 
          "nova::keystone::auth_placement::public_url": "http://192.168.24.7:8778/placement", 
          "cinder::cron::db_purge::month": "*", 
          "nova::host": "%{::fqdn}", 
          "glance_notifier_strategy": "noop", 
          "gnocchi::keystone::auth::internal_url": "http://192.168.24.7:8041", 
          "keystone::db::mysql::dbname": "keystone", 
          "tripleo.panko_api.firewall_rules": {
            "140 panko-api": {
              "dport": [
                8977, 
                13977
              ]
            }
          }, 
          "tripleo.nova_vnc_proxy.firewall_rules": {
            "137 nova_vnc_proxy": {
              "dport": [
                6080, 
                13080
              ]
            }
          }, 
          "tripleo::profile::base::swift::proxy::tls_proxy_bind_ip": "192.168.24.12", 
          "redis::sentinel::master_name": "%{hiera('bootstrap_nodeid')}", 
          "tripleo::profile::base::swift::proxy::tls_proxy_fqdn": "%{hiera('fqdn_storage')}", 
          "neutron::server::notifications::endpoint_type": "internal", 
          "ceilometer::keystone::authtoken::project_name": "service", 
          "neutron::rabbit_password": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "neutron::agents::ml2::ovs::l2_population": "False", 
          "horizon::horizon_ca": "/etc/ipa/ca.crt", 
          "ceilometer::agent::auth::auth_url": "http://192.168.24.7:5000", 
          "swift::storage::all::incoming_chmod": "Du=rwx,g=rx,o=rx,Fu=rw,g=r,o=r", 
          "tripleo.haproxy.firewall_rules": {
            "107 haproxy stats": {
              "dport": 1993
            }
          }, 
          "heat::wsgi::apache_api::ssl": false, 
          "horizon::bind_address": "192.168.24.12", 
          "tripleo::profile::base::database::mysql::client::ssl_ca": "/etc/ipa/ca.crt", 
          "tripleo::profile::base::gnocchi::api::gnocchi_backend": "swift", 
          "nova::placement::password": "ePtbUHubCqvanb7c8da8AAupz", 
          "keystone::endpoint::region": "regionOne", 
          "neutron::keystone::auth::internal_url": "http://192.168.24.7:9696", 
          "keystone::notification_format": "basic", 
          "tripleo.nova_api.firewall_rules": {
            "113 nova_api": {
              "dport": [
                8774, 
                13774, 
                8775
              ]
            }
          }, 
          "swift::proxy::port": "8080", 
          "tripleo::profile::base::keystone::extra_notification_topics": [], 
          "aodh::rabbit_password": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "cinder::keystone::auth::password": "ktePY6hcQhgyxk4KgZjgz78by", 
          "heat::keystone::authtoken::project_domain_name": "Default", 
          "apache::default_vhost": false, 
          "cinder::keystone::auth::admin_url": "http://192.168.24.7:8776/v1/%(tenant_id)s", 
          "heat::debug": false, 
          "heat::db::database_max_retries": -1, 
          "tripleo.neutron_ovs_agent.firewall_rules": {
            "136 neutron gre networks": {
              "proto": "gre"
            }, 
            "118 neutron vxlan networks": {
              "dport": 4789, 
              "proto": "udp"
            }
          }, 
          "panko::api::host": "%{hiera('fqdn_internal_api')}", 
          "cinder::rabbit_port": 5672, 
          "gnocchi::statsd::archive_policy_name": "low", 
          "glance::notify::rabbitmq::notification_driver": "messagingv2", 
          "horizon::disallow_iframe_embed": true, 
          "redis::ulimit": "10240", 
          "aodh::wsgi::apache::bind_host": "192.168.24.12", 
          "tripleo::profile::base::pacemaker::remote_authkey": "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", 
          "ceilometer::rabbit_use_ssl": "False", 
          "cinder::cron::db_purge::monthday": "*", 
          "keystone::policy::policies": {}, 
          "keystone::cron::token_flush::monthday": [
            "*"
          ], 
          "heat::keystone::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "rabbitmq::delete_guest_user": false, 
          "heat::keystone::authtoken::auth_url": "http://192.168.24.7:5000", 
          "aodh::rabbit_port": 5672, 
          "keystone::rabbit_heartbeat_timeout_threshold": 60, 
          "horizon::vhost_extra_params": {
            "priority": 10, 
            "access_log_format": "%a %l %u %t \\\"%r\\\" %>s %b \\\"%%{}{Referer}i\\\" \\\"%%{}{User-Agent}i\\\"", 
            "options": [
              "FollowSymLinks", 
              "MultiViews"
            ], 
            "add_listen": true
          }, 
          "aodh::rabbit_use_ssl": "False", 
          "heat::engine::auth_encryption_key": "T2CWHRsPCr29Gy9Hy3jbKWbvpJ2Y38Wp", 
          "ceilometer::keystone::auth::password": "FtXc7xgX8w2EJbXcCe3psFzv4", 
          "glance::api::sync_db": false, 
          "glance::keystone::auth::password": "wEWkucKnWXgaXbW3K73Z3s2Pa", 
          "heat::keystone::auth::internal_url": "http://192.168.24.7:8004/v1/%(tenant_id)s", 
          "nova::db::mysql_placement::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "keystone::endpoint::internal_url": "http://192.168.24.7:5000", 
          "nova::wsgi::apache_placement::ssl": false, 
          "nova::wsgi::apache_api::ssl": false, 
          "ceilometer::agent::polling::manage_polling": false, 
          "glance::api::enable_v1_api": false, 
          "apache::mod::prefork::serverlimit": 256, 
          "cinder::wsgi::apache::workers": "%{::os_workers}", 
          "tripleo::profile::pacemaker::cinder::volume_bundle::docker_environment": [
            "KOLLA_CONFIG_STRATEGY=COPY_ALWAYS"
          ], 
          "neutron::agents::ml2::ovs::arp_responder": false, 
          "horizon::keystone_url": "http://192.168.24.7:5000", 
          "rabbitmq::ssl_erl_dist": false, 
          "nova::db::mysql::password": "ePtbUHubCqvanb7c8da8AAupz", 
          "ceilometer::agent::notification::pipeline_publishers": [
            "gnocchi://"
          ], 
          "tripleo::profile::base::database::redis::tls_proxy_bind_ip": "192.168.24.12", 
          "neutron::plugins::ml2::mechanism_drivers": [
            "openvswitch"
          ], 
          "heat::keystone::domain::domain_name": "heat_stack", 
          "heat::keystone_ec2_uri": "http://192.168.24.7:5000/v3/ec2tokens", 
          "tripleo::profile::base::database::redis::tls_proxy_port": 6379, 
          "horizon::password_validator_help": "", 
          "redis::port": 6379, 
          "tripleo::firewall::purge_firewall_rules": false, 
          "heat::keystone::domain::domain_admin_email": "heat_stack_domain_admin@localhost", 
          "keystone::admin_token": "dCXDFYukP6EHbrTRude7dv6JC", 
          "neutron::bind_host": "192.168.24.12", 
          "gnocchi::statsd::project_id": "6c38cd8d-099a-4cb2-aecf-17be688e8616", 
          "keystone::roles::admin::service_tenant": "service", 
          "neutron::keystone::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "neutron::keystone::authtoken::auth_url": "http://192.168.24.7:5000", 
          "ceilometer::keystone::auth::configure_endpoint": false, 
          "swift::proxy::workers": "auto", 
          "cinder::api::service_name": "httpd", 
          "redis::bind": "192.168.24.12", 
          "heat::yaql_limit_iterators": 1000, 
          "nova::glance_api_servers": "http://192.168.24.7:9292", 
          "nova::db::mysql_api::setup_cell0": true, 
          "nova::debug": false, 
          "cinder::database_connection": "mysql+pymysql://cinder:ktePY6hcQhgyxk4KgZjgz78by@192.168.24.7/cinder?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "nova::rabbit_heartbeat_timeout_threshold": 60, 
          "swift::storage::all::container_pipeline": [
            "healthcheck", 
            "container-server"
          ], 
          "tripleo::profile::base::swift::ringbuilder::replicas": 3, 
          "panko::debug": false, 
          "glance::db::mysql::host": "192.168.24.7", 
          "heat::api_cfn::service_name": "httpd", 
          "nova::keystone::auth_placement::password": "ePtbUHubCqvanb7c8da8AAupz", 
          "tripleo::profile::base::swift::proxy::rabbit_port": 5672, 
          "cinder::keystone::authtoken::password": "ktePY6hcQhgyxk4KgZjgz78by", 
          "gnocchi::keystone::authtoken::project_domain_name": "Default", 
          "aodh::keystone::authtoken::user_domain_name": "Default", 
          "tripleo::profile::base::heat::manage_db_purge": true, 
          "cinder::keystone::auth::admin_url_v2": "http://192.168.24.7:8776/v2/%(tenant_id)s", 
          "glance::api::authtoken::auth_url": "http://192.168.24.7:5000", 
          "nova::db::mysql_placement::host": "192.168.24.7", 
          "glance::api::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "nova::db::mysql::user": "nova", 
          "neutron::server::notifications::password": "ePtbUHubCqvanb7c8da8AAupz", 
          "panko::api::event_time_to_live": "86400", 
          "rabbitmq::package_provider": "yum", 
          "heat::keystone::auth_cfn::internal_url": "http://192.168.24.7:8000/v1", 
          "tripleo::firewall::manage_firewall": true, 
          "keystone::enable_proxy_headers_parsing": true, 
          "nova::my_ip": "192.168.24.12", 
          "nova::placement::auth_url": "http://192.168.24.7:5000", 
          "cinder::api::enable_proxy_headers_parsing": true, 
          "hacluster_pwd": "CPVZWQEedTJ3eWyV", 
          "pacemaker::resource_defaults::defaults": {
            "resource-stickiness": {
              "value": "INFINITY"
            }
          }, 
          "ceilometer::db::mysql::user": "ceilometer", 
          "nova::cron::archive_deleted_rows::user": "nova", 
          "nova::wsgi::apache_placement::servername": "%{hiera('fqdn_internal_api')}", 
          "glance::keystone::auth::tenant": "service", 
          "neutron::global_physnet_mtu": 1500, 
          "ceilometer::rabbit_userid": "guest", 
          "panko::keystone::auth::password": "aCVWZBffGWdcKuYpGjK6JN8YB", 
          "swift::storage::all::account_pipeline": [
            "healthcheck", 
            "account-server"
          ], 
          "gnocchi::keystone::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "gnocchi::keystone::authtoken::auth_url": "http://192.168.24.7:5000", 
          "tripleo::profile::base::docker::network_options": "--bip=172.31.0.1/24", 
          "rabbitmq::default_pass": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "ceilometer::keystone::authtoken::auth_url": "http://192.168.24.7:5000", 
          "ceilometer::keystone::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "heat::keystone::auth::public_url": "http://192.168.24.7:8004/v1/%(tenant_id)s", 
          "ceilometer::dispatcher::gnocchi::resources_definition_file": "gnocchi_resources.yaml", 
          "gnocchi::storage::s3::s3_secret_access_key": "", 
          "glance::api::show_image_direct_url": true, 
          "aodh::wsgi::apache::wsgi_process_display_name": "aodh_wsgi", 
          "kernel_modules": {
            "nf_conntrack": {}, 
            "nf_conntrack_proto_sctp": {}
          }, 
          "heat::wsgi::apache_api_cfn::bind_host": "192.168.24.12", 
          "tripleo::profile::base::swift::ringbuilder::swift_ring_put_tempurl": "http://192.168.24.1:8080/v1/AUTH_717af5f776724399988f7a1f32c8c7a5/overcloud-swift-rings/swift-rings.tar.gz?temp_url_sig=422eb65f32e4a45245faa8d5bfd19899b9aa3e6c&temp_url_expires=1520709619", 
          "panko::api::enable_proxy_headers_parsing": true, 
          "heat::engine::plugin_dirs": [], 
          "keystone::service_name": "httpd", 
          "keystone::cron::token_flush::hour": [
            "*"
          ], 
          "tripleo::profile::pacemaker::haproxy_bundle::internal_keys_directory": "/etc/pki/tls/private/haproxy", 
          "tripleo::profile::base::sshd::motd": "", 
          "cinder::db::database_max_retries": -1, 
          "neutron::server::enable_dvr": false, 
          "tripleo::profile::base::docker::docker_options": "--log-driver=journald --signature-verification=false --iptables=false --live-restore", 
          "neutron::agents::metadata::metadata_protocol": "http", 
          "gnocchi::db::mysql::host": "192.168.24.7", 
          "apache::server_tokens": "Prod", 
          "keystone::cron::token_flush::maxdelay": 0, 
          "horizon::disable_password_reveal": true, 
          "neutron::server::notifications::tenant_name": "service", 
          "neutron::agents::ml2::ovs::extensions": [
            "qos"
          ], 
          "tripleo.rabbitmq.firewall_rules": {
            "109 rabbitmq-bundle": {
              "dport": [
                3122, 
                4369, 
                5672, 
                25672
              ]
            }
          }, 
          "ceilometer::agent::auth::auth_user_domain_name": "Default", 
          "aodh::keystone::authtoken::password": "zpRBy7zcejYfAbksu9YRVMfhf", 
          "cinder::db::mysql::dbname": "cinder", 
          "aodh::db::mysql::password": "zpRBy7zcejYfAbksu9YRVMfhf", 
          "nova::keystone::auth::tenant": "service", 
          "tripleo::profile::pacemaker::database::mysql_bundle::control_port": 3123, 
          "swift::proxy::authtoken::password": "hw46VkJ6ZuYX88bGnPYm7gwqt", 
          "heat::engine::convergence_engine": true, 
          "tripleo.heat_api.firewall_rules": {
            "125 heat_api": {
              "dport": [
                8004, 
                13004
              ]
            }
          }, 
          "glance::api::debug": false, 
          "keystone::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "aodh::auth::auth_password": "zpRBy7zcejYfAbksu9YRVMfhf", 
          "ceilometer::keystone::auth::internal_url": "http://192.168.24.7:8777", 
          "keystone::wsgi::apache::admin_bind_host": "192.168.24.12", 
          "tripleo::profile::base::database::redis::tls_proxy_fqdn": "%{hiera('fqdn_internal_api')}", 
          "nova::db::mysql_api::host": "192.168.24.7", 
          "horizon::allowed_hosts": [
            "*"
          ], 
          "neutron::db::sync::extra_params": "", 
          "gnocchi::keystone::auth::admin_url": "http://192.168.24.7:8041", 
          "nova::cron::archive_deleted_rows::max_rows": "100", 
          "apache::mod::prefork::maxclients": 256, 
          "panko::auth::auth_url": "http://192.168.24.7:5000", 
          "nova::keystone::auth_placement::internal_url": "http://192.168.24.7:8778/placement", 
          "tripleo::haproxy::redis_password": "DRMmzyDvmMuhG2AytUCHfpKTW", 
          "gnocchi::storage::ceph::ceph_conffile": "/etc/ceph/ceph.conf", 
          "nova::keystone::authtoken::auth_url": "http://192.168.24.7:35357", 
          "heat::cron::purge_deleted::ensure": "present", 
          "panko::db::mysql::dbname": "panko", 
          "nova::keystone::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "neutron::agents::dhcp::enable_force_metadata": false, 
          "keystone::roles::admin::email": "admin@example.com", 
          "enable_galera": true, 
          "swift::keystone::auth::operator_roles": [
            "admin", 
            "swiftoperator", 
            "ResellerAdmin"
          ], 
          "memcached_ipv6": false, 
          "nova::network::neutron::neutron_url": "http://192.168.24.7:9696", 
          "keystone_ssl_certificate_key": "", 
          "glance::policy::policies": {}, 
          "nova::db::mysql_placement::password": "ePtbUHubCqvanb7c8da8AAupz", 
          "horizon::enable_secure_proxy_ssl_header": true, 
          "glance::api::database_connection": "mysql+pymysql://glance:wEWkucKnWXgaXbW3K73Z3s2Pa@192.168.24.7/glance?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "glance::api::authtoken::project_name": "service", 
          "tripleo.neutron_api.firewall_rules": {
            "114 neutron api": {
              "dport": [
                9696, 
                13696
              ]
            }
          }, 
          "neutron::plugins::ml2::flat_networks": [
            "datacentre"
          ], 
          "tripleo.glance_api.firewall_rules": {
            "112 glance_api": {
              "dport": [
                9292, 
                13292
              ]
            }
          }, 
          "nova::network::neutron::neutron_region_name": "regionOne", 
          "neutron::agents::l3::external_network_bridge": "", 
          "glance::keystone::auth::internal_url": "http://192.168.24.7:9292", 
          "glance::backend::swift::swift_store_create_container_on_put": true, 
          "horizon::servername": "%{hiera('fqdn_internal_api')}", 
          "tripleo.neutron_l3.firewall_rules": {
            "106 neutron_l3 vrrp": {
              "proto": "vrrp"
            }
          }, 
          "neutron::server::database_connection": "mysql+pymysql://neutron:22PeyXGFu7qJevbd3VtKnTeh3@192.168.24.7/ovs_neutron?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "cinder::wsgi::apache::bind_host": "192.168.24.12", 
          "tripleo::profile::base::database::mysql::bind_address": "%{hiera('fqdn_internal_api')}", 
          "swift::storage::all::object_pipeline": [
            "healthcheck", 
            "recon", 
            "object-server"
          ], 
          "redis::notify_service": false, 
          "gnocchi::policy::policies": {}, 
          "ceilometer_redis_password": "DRMmzyDvmMuhG2AytUCHfpKTW", 
          "nova::scheduler::filter::scheduler_available_filters": [], 
          "cinder::keystone::authtoken::project_domain_name": "Default", 
          "neutron::keystone::auth::region": "regionOne", 
          "aodh::policy::policies": {}, 
          "keystone_enable_member": true, 
          "neutron::agents::ml2::ovs::bridge_mappings": [
            "datacentre:br-ex"
          ], 
          "cinder::keystone::auth::admin_url_v3": "http://192.168.24.7:8776/v3/%(tenant_id)s", 
          "neutron::agents::metadata::auth_tenant": "service", 
          "tripleo.swift_storage.firewall_rules": {
            "123 swift storage": {
              "dport": [
                873, 
                6000, 
                6001, 
                6002
              ]
            }
          }, 
          "nova::keystone::auth_placement::tenant": "service", 
          "neutron::keystone::authtoken::project_name": "service", 
          "horizon::secure_cookies": false, 
          "nova::wsgi::apache_placement::api_port": "8778", 
          "keystone::rabbit_port": 5672, 
          "tripleo::profile::pacemaker::database::redis_bundle::control_port": 3124, 
          "swift::storage::all::storage_local_net_ip": "192.168.24.12", 
          "nova::use_ipv6": false, 
          "cinder::api::nova_catalog_admin_info": "compute:nova:adminURL", 
          "keystone::rabbit_userid": "guest", 
          "heat::keystone::authtoken::project_name": "service", 
          "aodh::keystone::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "panko::expirer::minute": "1", 
          "neutron::debug": false, 
          "heat::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "tripleo::glance::nfs_mount::edit_fstab": false, 
          "tripleo::profile::base::cinder::volume::iscsi::cinder_iscsi_address": "192.168.24.12", 
          "neutron::rabbit_port": 5672, 
          "tripleo::haproxy::haproxy_log_address": "/dev/log", 
          "neutron::rabbit_user": "guest", 
          "apache::server_signature": "Off", 
          "rabbitmq::file_limit": 65536, 
          "glance::notify::rabbitmq::rabbit_use_ssl": "False", 
          "panko::wsgi::apache::ssl": false, 
          "tripleo::profile::base::glance::api::tls_proxy_fqdn": "%{hiera('fqdn_internal_api')}", 
          "gnocchi_redis_password": "DRMmzyDvmMuhG2AytUCHfpKTW", 
          "nova::keystone::authtoken::project_name": "service", 
          "enable_fencing": false, 
          "neutron::agents::dhcp::enable_isolated_metadata": false, 
          "tripleo::profile::pacemaker::haproxy_bundle::tls_mapping": [
            "/etc/ipa/ca.crt", 
            "/etc/pki/tls/private/haproxy", 
            "/etc/pki/tls/certs/haproxy", 
            "/etc/pki/tls/private/overcloud_endpoint.pem"
          ], 
          "panko::db::mysql::password": "aCVWZBffGWdcKuYpGjK6JN8YB", 
          "neutron::rabbit_use_ssl": "False", 
          "neutron::plugins::ml2::tunnel_id_ranges": [
            "1:4094"
          ], 
          "neutron::keystone::authtoken::project_domain_name": "Default", 
          "ceilometer::agent::auth::auth_endpoint_type": "internalURL", 
          "glance::notify::rabbitmq::rabbit_userid": "guest", 
          "neutron::core_plugin": "ml2", 
          "heat::rpc_response_timeout": 600, 
          "nova::keystone::authtoken::user_domain_name": "Default", 
          "gnocchi::storage::s3::s3_access_key_id": "", 
          "glance::api::pipeline": "keystone", 
          "neutron::notification_driver": "messagingv2", 
          "rabbitmq::ssl_only": false, 
          "tripleo.heat_api_cfn.firewall_rules": {
            "125 heat_cfn": {
              "dport": [
                8000, 
                13800
              ]
            }
          }, 
          "neutron::db::database_max_retries": -1, 
          "tripleo::trusted_cas::ca_map": {}, 
          "tripleo::profile::base::keystone::ceilometer_notification_topics": [
            "notifications"
          ], 
          "tripleo::profile::base::swift::ringbuilder::part_power": 10, 
          "gnocchi::storage::s3::s3_endpoint_url": "", 
          "nova::keystone::auth::admin_url": "http://192.168.24.7:8774/v2.1", 
          "neutron::plugins::ml2::extension_drivers": [
            "qos", 
            "port_security"
          ], 
          "cinder::keystone::auth::public_url_v2": "http://192.168.24.7:8776/v2/%(tenant_id)s", 
          "cinder::keystone::auth::public_url_v3": "http://192.168.24.7:8776/v3/%(tenant_id)s", 
          "glance::backend::swift::swift_store_auth_version": 3, 
          "aodh::keystone::auth::internal_url": "http://192.168.24.7:8042", 
          "nova::db::mysql::dbname": "nova", 
          "heat::keystone::auth::password": "HBqZMnZMW7ZhRJhEh3kqBT6BT", 
          "keystone::notification_driver": "messagingv2", 
          "ceilometer::keystone::auth::tenant": "service", 
          "neutron::policy::policies": {}, 
          "cinder::debug": false, 
          "nova::cron::archive_deleted_rows::until_complete": false, 
          "nova::scheduler::filter::scheduler_default_filters": [], 
          "rabbitmq::port": 5672, 
          "tripleo::profile::base::tuned::profile": "", 
          "neutron::agents::ml2::ovs::enable_distributed_routing": false, 
          "aodh::keystone::authtoken::auth_url": "http://192.168.24.7:5000", 
          "tripleo::profile::pacemaker::database::redis_bundle::tls_proxy_bind_ip": "192.168.24.12", 
          "neutron::db::mysql::host": "192.168.24.7", 
          "keystone::admin_bind_host": "%{hiera('fqdn_ctlplane')}", 
          "tripleo::profile::base::swift::storage::enable_swift_storage": true, 
          "tripleo.swift_proxy.firewall_rules": {
            "122 swift proxy": {
              "dport": [
                8080, 
                13808
              ]
            }
          }, 
          "tripleo::profile::pacemaker::database::mysql::gmcast_listen_addr": "192.168.24.12", 
          "neutron::keystone::auth::admin_url": "http://192.168.24.7:9696", 
          "tripleo::profile::base::cinder::cinder_enable_db_purge": true, 
          "tripleo::profile::pacemaker::rabbitmq_bundle::control_port": 3122, 
          "keystone::cron::token_flush::user": "keystone", 
          "nova::cron::archive_deleted_rows::month": "*", 
          "redis::masterauth": "DRMmzyDvmMuhG2AytUCHfpKTW", 
          "keystone::admin_password": "CTU9zEJAv2ncR2nujhbvZJwsp", 
          "neutron::agents::metadata::debug": false, 
          "gnocchi::storage::swift::swift_authurl": "http://192.168.24.7:5000/v3", 
          "tripleo::profile::base::swift::ringbuilder:skip_consistency_check": true, 
          "ceilometer::dispatcher::gnocchi::filter_project": "service", 
          "glance::keystone::auth::public_url": "http://192.168.24.7:9292", 
          "glance::backend::swift::swift_store_user": "service:glance", 
          "aodh::rabbit_userid": "guest", 
          "enable_panko_expirer": true, 
          "neutron::agents::dhcp::debug": false, 
          "panko::api::service_name": "httpd", 
          "panko::wsgi::apache::bind_host": "192.168.24.12", 
          "ceilometer::db::mysql::password": "FtXc7xgX8w2EJbXcCe3psFzv4", 
          "nova::cron::archive_deleted_rows::destination": "/var/log/nova/nova-rowsflush.log", 
          "sysctl_settings": {
            "net.ipv4.conf.all.arp_accept": {
              "value": 1
            }, 
            "net.ipv6.conf.default.autoconf": {
              "value": 0
            }, 
            "net.ipv6.conf.default.accept_redirects": {
              "value": 0
            }, 
            "net.ipv4.ip_forward": {
              "value": 1
            }, 
            "net.nf_conntrack_max": {
              "value": 500000
            }, 
            "fs.inotify.max_user_instances": {
              "value": 1024
            }, 
            "net.ipv4.conf.default.log_martians": {
              "value": 1
            }, 
            "net.ipv4.conf.all.send_redirects": {
              "value": 0
            }, 
            "net.ipv4.conf.all.secure_redirects": {
              "value": 0
            }, 
            "net.netfilter.nf_conntrack_max": {
              "value": 500000
            }, 
            "net.ipv6.conf.all.autoconf": {
              "value": 0
            }, 
            "net.ipv4.tcp_keepalive_probes": {
              "value": 5
            }, 
            "kernel.pid_max": {
              "value": 1048576
            }, 
            "net.ipv4.conf.all.log_martians": {
              "value": 1
            }, 
            "fs.suid_dumpable": {
              "value": 0
            }, 
            "net.ipv4.conf.default.accept_redirects": {
              "value": 0
            }, 
            "net.ipv6.conf.all.accept_ra": {
              "value": 0
            }, 
            "net.ipv4.conf.default.secure_redirects": {
              "value": 0
            }, 
            "net.ipv4.tcp_keepalive_intvl": {
              "value": 1
            }, 
            "net.ipv4.tcp_keepalive_time": {
              "value": 5
            }, 
            "net.ipv6.conf.default.accept_ra": {
              "value": 0
            }, 
            "kernel.dmesg_restrict": {
              "value": 1
            }, 
            "net.ipv6.conf.all.accept_redirects": {
              "value": 0
            }, 
            "net.ipv6.conf.all.disable_ipv6": {
              "value": 0
            }, 
            "net.core.netdev_max_backlog": {
              "value": 10000
            }, 
            "net.ipv4.neigh.default.gc_thresh1": {
              "value": 1024
            }, 
            "net.ipv4.neigh.default.gc_thresh2": {
              "value": 2048
            }, 
            "net.ipv4.neigh.default.gc_thresh3": {
              "value": 4096
            }, 
            "net.ipv4.conf.default.send_redirects": {
              "value": 0
            }, 
            "net.ipv6.conf.default.disable_ipv6": {
              "value": 0
            }
          }, 
          "central_namespace": true, 
          "cinder::api::bind_host": "%{hiera('fqdn_internal_api')}", 
          "nova::cron::archive_deleted_rows::minute": "1", 
          "swift::proxy::pipeline": [
            "catch_errors", 
            "healthcheck", 
            "proxy-logging", 
            "cache", 
            "ratelimit", 
            "bulk", 
            "tempurl", 
            "formpost", 
            "authtoken", 
            "keystone", 
            "staticweb", 
            "copy", 
            "container_quotas", 
            "account_quotas", 
            "slo", 
            "dlo", 
            "versioned_writes", 
            "proxy-logging", 
            "proxy-server"
          ], 
          "ceilometer::agent::auth::auth_password": "FtXc7xgX8w2EJbXcCe3psFzv4", 
          "apache::ip": "192.168.24.12", 
          "tripleo::glance::nfs_mount::share": "", 
          "nova::placement::os_interface": "internal", 
          "glance_log_file": "", 
          "neutron::db::database_db_max_retries": -1, 
          "heat::cron::purge_deleted::monthday": "*", 
          "heat::yaql_memory_quota": 100000, 
          "ceilometer::rabbit_heartbeat_timeout_threshold": 60, 
          "swift::proxy::account_autocreate": true, 
          "gnocchi::api::enabled": true, 
          "cinder::wsgi::apache::servername": "%{hiera('fqdn_internal_api')}", 
          "heat::keystone::auth::tenant": "service", 
          "gnocchi::storage::s3::s3_region_name": "", 
          "tripleo::profile::base::cinder::volume::nfs::cinder_nas_secure_file_permissions": "False", 
          "tripleo::profile::base::swift::proxy::ceilometer_messaging_use_ssl": "False", 
          "keystone::cron::token_flush::destination": "/var/log/keystone/keystone-tokenflush.log", 
          "nova::placement::os_region_name": "regionOne", 
          "pacemaker::corosync::manage_fw": false, 
          "glance::db::mysql::user": "glance", 
          "gnocchi::storage::swift::swift_user": "service:gnocchi", 
          "heat::engine::heat_waitcondition_server_url": "http://192.168.24.7:8000/v1/waitcondition", 
          "heat::db::database_db_max_retries": -1, 
          "horizon::password_validator": "", 
          "cinder::cron::db_purge::destination": "/var/log/cinder/cinder-rowsflush.log", 
          "rabbitmq::ssl": false, 
          "glance::keystone::authtoken::user_domain_name": "Default", 
          "ntp::iburst_enable": true, 
          "keystone::wsgi::apache::workers": "%{::os_workers}", 
          "glance::keystone::auth::region": "regionOne", 
          "nova::network::neutron::neutron_auth_url": "http://192.168.24.7:35357/v3", 
          "nova::db::mysql_api::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "heat::rabbit_use_ssl": "False", 
          "heat::cron::purge_deleted::age_type": "days", 
          "ntp::servers": [
            "clock.redhat.com"
          ], 
          "tripleo::profile::base::keystone::heat_admin_user": "heat_stack_domain_admin", 
          "ceilometer::snmpd_readonly_username": "ro_snmp_user", 
          "neutron::agents::dhcp::dnsmasq_dns_servers": [], 
          "nova::database_connection": "mysql+pymysql://nova:ePtbUHubCqvanb7c8da8AAupz@192.168.24.7/nova?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "gnocchi::db::sync::extra_opts": " --sacks-number 128", 
          "neutron::server::enable_proxy_headers_parsing": true, 
          "pacemaker::corosync::cluster_name": "tripleo_cluster", 
          "heat::wsgi::apache_api_cfn::ssl": false, 
          "panko::keystone::auth::internal_url": "http://192.168.24.7:8977", 
          "swift::swift_hash_path_suffix": "gckQDd7u4nVbzyejER7hhwXfg", 
          "aodh::auth::auth_region": "regionOne", 
          "heat::cron::purge_deleted::maxdelay": "3600", 
          "aodh::api::gnocchi_external_project_owner": "service", 
          "tripleo::profile::base::swift::proxy::ceilometer_enabled": false, 
          "swift::storage::disks::args": {}, 
          "cinder::keystone::auth::internal_url": "http://192.168.24.7:8776/v1/%(tenant_id)s", 
          "nova::network::neutron::neutron_ovs_bridge": "br-int", 
          "nova::db::mysql_api::password": "ePtbUHubCqvanb7c8da8AAupz", 
          "cinder::keystone::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "neutron::agents::metadata::shared_secret": "B6PVNE42RgMZznXdCEW3wZxWV", 
          "panko::db::database_connection": "mysql+pymysql://panko:aCVWZBffGWdcKuYpGjK6JN8YB@192.168.24.7/panko?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "cinder::keystone::authtoken::auth_url": "http://192.168.24.7:5000", 
          "tripleo::profile::base::sshd::options": {
            "Subsystem": "sftp  /usr/libexec/openssh/sftp-server", 
            "UsePAM": "yes", 
            "UsePrivilegeSeparation": "sandbox", 
            "GSSAPICleanupCredentials": "no", 
            "SyslogFacility": "AUTHPRIV", 
            "GSSAPIAuthentication": "yes", 
            "PasswordAuthentication": "no", 
            "AuthorizedKeysFile": ".ssh/authorized_keys", 
            "AcceptEnv": [
              "LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES", 
              "LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT", 
              "LC_IDENTIFICATION LC_ALL LANGUAGE", 
              "XMODIFIERS"
            ], 
            "UseDNS": "no", 
            "HostKey": [
              "/etc/ssh/ssh_host_rsa_key", 
              "/etc/ssh/ssh_host_ecdsa_key", 
              "/etc/ssh/ssh_host_ed25519_key"
            ], 
            "X11Forwarding": "yes", 
            "ChallengeResponseAuthentication": "no"
          }, 
          "neutron::server::notifications::auth_url": "http://192.168.24.7:5000", 
          "keystone::fernet_keys": {
            "/etc/keystone/fernet-keys/0": {
              "content": "AC-W5uHRJZwAyaGkiU8CYuCqIHN45Ax75_Py2gZYxes="
            }, 
            "/etc/keystone/fernet-keys/1": {
              "content": "4rmIUuAlJenbykCqx_L-MxfsSY9pHkMLSg8cdyVlDi8="
            }
          }, 
          "heat::cron::purge_deleted::destination": "/dev/null", 
          "tripleo::profile::base::keystone::heat_admin_email": "heat_stack_domain_admin@localhost", 
          "heat::engine::configure_delegated_roles": false, 
          "swift::proxy::staticweb::url_base": "http://192.168.24.7:8080", 
          "gnocchi::storage::ceph::ceph_username": "openstack", 
          "swift::proxy::proxy_local_net_ip": "192.168.24.12", 
          "nova_enable_db_purge": true, 
          "nova::vncproxy::common::vncproxy_port": "6080", 
          "cinder::rabbit_use_ssl": "False", 
          "rabbitmq_environment": {
            "export ERL_EPMD_ADDRESS": "%{hiera('rabbitmq::interface')}", 
            "NODE_IP_ADDRESS": "", 
            "RABBITMQ_NODENAME": "rabbit@%{::hostname}", 
            "RABBITMQ_SERVER_ERL_ARGS": "\"+K true +P 1048576 -kernel inet_default_connect_options [{nodelay,true}]\"", 
            "NODE_PORT": ""
          }, 
          "nova::cron::archive_deleted_rows::weekday": "*", 
          "nova::purge_config": false, 
          "heat::rabbit_heartbeat_timeout_threshold": 60, 
          "cinder::policy::policies": {}, 
          "keystone::endpoint::public_url": "http://192.168.24.7:5000", 
          "swift::storage::all::mount_check": false, 
          "neutron::plugins::ml2::overlay_ip_version": 4, 
          "tripleo::profile::base::docker::debug": false, 
          "nova::vncproxy::enabled": true, 
          "ceilometer::agent::auth::auth_tenant_name": "service", 
          "tripleo.pacemaker.firewall_rules": {
            "131 pacemaker udp": {
              "dport": 5405, 
              "proto": "udp"
            }, 
            "130 pacemaker tcp": {
              "dport": [
                2224, 
                3121, 
                21064
              ], 
              "proto": "tcp"
            }
          }, 
          "nova::notification_driver": "messagingv2", 
          "cinder::cron::db_purge::weekday": "*", 
          "neutron::agents::ml2::ovs::tunnel_types": [
            "vxlan"
          ], 
          "heat::keystone::authtoken::user_domain_name": "Default", 
          "gnocchi::wsgi::apache::wsgi_process_display_name": "gnocchi_wsgi", 
          "nova::db::mysql_api::user": "nova_api", 
          "rabbitmq::repos_ensure": false, 
          "nova::db::mysql_api::dbname": "nova_api", 
          "neutron::agents::dhcp::enable_metadata_network": false, 
          "nova::api::service_name": "httpd", 
          "tripleo::profile::base::cinder::volume::rbd::cinder_rbd_extra_pools": [], 
          "cinder::db::mysql::user": "cinder", 
          "swift::proxy::node_timeout": 60, 
          "tripleo.horizon.firewall_rules": {
            "127 horizon": {
              "dport": [
                80, 
                443
              ]
            }
          }, 
          "gnocchi::storage::ceph::ceph_keyring": "/etc/ceph/ceph.client.openstack.keyring", 
          "heat::rabbit_password": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "gnocchi::db::database_connection": "mysql+pymysql://gnocchi:R6Yn6NmnpN7nAfGMJfmmcsCRZ@192.168.24.7/gnocchi?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "neutron::db::mysql::user": "neutron", 
          "cinder::keystone::auth::region": "regionOne", 
          "swift::keystone::auth::admin_url_s3": "http://192.168.24.7:8080", 
          "nova::keystone::auth::password": "ePtbUHubCqvanb7c8da8AAupz", 
          "gnocchi::keystone::authtoken::user_domain_name": "Default", 
          "panko::keystone::auth::admin_url": "http://192.168.24.7:8977", 
          "ceilometer::keystone::auth::region": "regionOne", 
          "glance::backend::rbd::rbd_store_user": "openstack", 
          "tripleo::profile::base::database::mysql::client::mysql_client_bind_address": "192.168.24.12", 
          "heat::wsgi::apache_api::bind_host": "192.168.24.12", 
          "aodh::keystone::auth::password": "zpRBy7zcejYfAbksu9YRVMfhf", 
          "neutron::plugins::ml2::tenant_network_types": [
            "vxlan"
          ], 
          "gnocchi::storage::swift::swift_key": "R6Yn6NmnpN7nAfGMJfmmcsCRZ", 
          "aodh::wsgi::apache::servername": "%{hiera('fqdn_internal_api')}", 
          "keystone::fernet_max_active_keys": 5, 
          "tripleo::profile::base::cinder::volume::iscsi::cinder_lvm_loop_device_size": 10280, 
          "ceilometer::agent::notification::manage_pipeline": false, 
          "haproxy_docker": true, 
          "heat::max_json_body_size": 4194304, 
          "corosync_token_timeout": 10000, 
          "nova::notification_format": "unversioned", 
          "keystone::database_connection": "mysql+pymysql://keystone:dCXDFYukP6EHbrTRude7dv6JC@192.168.24.7/keystone?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "nova::keystone::auth_placement::region": "regionOne", 
          "aodh::notification_driver": "messagingv2", 
          "heat::api::service_name": "httpd", 
          "tripleo::haproxy::haproxy_stats": true, 
          "glance::backend::rbd::rbd_store_ceph_conf": "/etc/ceph/ceph.conf", 
          "swift::keystone::auth::internal_url_s3": "http://192.168.24.7:8080", 
          "keystone::token_provider": "fernet", 
          "nova::ram_allocation_ratio": "1.0", 
          "neutron::rabbit_heartbeat_timeout_threshold": 60, 
          "keystone::cron::token_flush::month": [
            "*"
          ], 
          "rabbitmq::wipe_db_on_cookie_change": true, 
          "neutron::keystone::auth::tenant": "service", 
          "heat::keystone::auth_cfn::region": "regionOne", 
          "tripleo::profile::base::neutron::server::tls_proxy_fqdn": "%{hiera('fqdn_internal_api')}", 
          "glance::api::image_member_quota": 128, 
          "nova::cron::archive_deleted_rows::monthday": "*", 
          "keystone::fernet_replace_keys": true, 
          "cinder::config": {
            "DEFAULT/swift_catalog_info": {
              "value": "object-store:swift:internalURL"
            }
          }, 
          "tripleo.snmp.firewall_rules": {
            "124 snmp": {
              "dport": 161, 
              "source": "%{hiera('snmpd_network')}", 
              "proto": "udp"
            }
          }, 
          "rabbitmq::tcp_keepalive": true, 
          "swift::storage::all::container_server_workers": "auto", 
          "glance::notify::rabbitmq::rabbit_password": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "nova::rabbit_userid": "guest", 
          "neutron::host": "%{::fqdn}", 
          "nova::api::default_floating_pool": "public", 
          "keystone::db::mysql::user": "keystone", 
          "tripleo::glance::nfs_mount::options": "_netdev,bg,intr,context=system_u:object_r:glance_var_lib_t:s0", 
          "neutron::agents::metadata::auth_password": "22PeyXGFu7qJevbd3VtKnTeh3", 
          "aodh::db::mysql::host": "192.168.24.7", 
          "cinder::host": "hostgroup", 
          "gnocchi::keystone::auth::region": "regionOne", 
          "cinder::glance::glance_api_servers": "http://192.168.24.7:9292", 
          "heat::db::mysql::host": "192.168.24.7", 
          "ceilometer::rabbit_password": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "keystone::wsgi::apache::ssl": false, 
          "tripleo::profile::pacemaker::database::mysql::bind_address": "%{hiera('fqdn_internal_api')}", 
          "panko::expirer::monthday": "*", 
          "nova::keystone::auth_placement::admin_url": "http://192.168.24.7:8778/placement", 
          "keystone::wsgi::apache::bind_host": "192.168.24.12", 
          "rabbitmq::ssl_port": 5672, 
          "gnocchi::statsd::resource_id": "0a8b55df-f90f-491c-8cb9-7cdecec6fc26", 
          "nova::api::api_bind_address": "%{hiera('fqdn_internal_api')}", 
          "swift::keystone::auth::public_url": "http://192.168.24.7:8080/v1/AUTH_%(tenant_id)s", 
          "panko::keystone::auth::region": "regionOne", 
          "heat::engine::trusts_delegated_roles": [], 
          "neutron::allow_overlapping_ips": true, 
          "neutron::keystone::auth::public_url": "http://192.168.24.7:9696", 
          "cinder::keystone::auth::tenant": "service", 
          "glance::db::mysql::password": "wEWkucKnWXgaXbW3K73Z3s2Pa", 
          "swift::storage::all::account_server_workers": "auto", 
          "glance::api::bind_host": "192.168.24.12", 
          "neutron::purge_config": false, 
          "nova::db::sync_api::db_sync_timeout": 300, 
          "neutron::dhcp_agent_notification": true, 
          "neutron::plugins::ml2::vni_ranges": [
            "1:4094"
          ], 
          "aodh::debug": false, 
          "glance::api::enable_proxy_headers_parsing": true, 
          "tripleo::profile::pacemaker::database::mysql::ca_file": "/etc/ipa/ca.crt", 
          "neutron::keystone::auth::password": "22PeyXGFu7qJevbd3VtKnTeh3", 
          "panko::keystone::authtoken::user_domain_name": "Default", 
          "gnocchi::keystone::authtoken::project_name": "service", 
          "heat::db::mysql::password": "HBqZMnZMW7ZhRJhEh3kqBT6BT", 
          "tripleo::profile::base::swift::storage::use_local_dir": true, 
          "ceilometer::db::mysql::dbname": "ceilometer", 
          "neutron::server::router_distributed": false, 
          "nova::api::enabled": true, 
          "gnocchi::api::service_name": "httpd", 
          "panko::keystone::authtoken::project_name": "service", 
          "nova::keystone::auth::region": "regionOne", 
          "tripleo::haproxy::haproxy_stats_user": "admin", 
          "tripleo::profile::base::swift::ringbuilder::build_ring": true, 
          "aodh_redis_password": "DRMmzyDvmMuhG2AytUCHfpKTW", 
          "glance::api::authtoken::password": "wEWkucKnWXgaXbW3K73Z3s2Pa", 
          "keystone::db::mysql::password": "dCXDFYukP6EHbrTRude7dv6JC", 
          "nova_wsgi_enabled": true, 
          "nova::api::instance_name_template": "instance-%08x", 
          "ceilometer::agent::notification::event_pipeline_publishers": [
            "gnocchi://", 
            "panko://"
          ], 
          "panko::auth::auth_password": "aCVWZBffGWdcKuYpGjK6JN8YB", 
          "tripleo::profile::base::cinder::volume::cinder_enable_rbd_backend": false, 
          "tripleo::profile::base::glance::api::tls_proxy_bind_ip": "192.168.24.12", 
          "glance::api::enable_v2_api": true, 
          "tripleo::profile::base::rabbitmq::enable_internal_tls": false, 
          "heat::keystone::domain::domain_admin": "heat_stack_domain_admin", 
          "nova::keystone::auth::public_url": "http://192.168.24.7:8774/v2.1", 
          "ceilometer::agent::auth::auth_region": "regionOne", 
          "gnocchi::storage::ceph::ceph_pool": "metrics", 
          "cinder::cron::db_purge::age": "0", 
          "neutron::db::mysql::dbname": "ovs_neutron", 
          "cinder::cron::db_purge::minute": "1", 
          "panko::auth::auth_region": "regionOne", 
          "rabbitmq::nr_ha_queues": -1, 
          "heat::cron::purge_deleted::user": "heat", 
          "nova::api::enable_proxy_headers_parsing": true, 
          "cinder::keystone::auth::internal_url_v2": "http://192.168.24.7:8776/v2/%(tenant_id)s", 
          "cinder::keystone::auth::internal_url_v3": "http://192.168.24.7:8776/v3/%(tenant_id)s", 
          "gnocchi::db::mysql::user": "gnocchi", 
          "ceilometer::keystone::auth::admin_url": "http://192.168.24.7:8777", 
          "ntp::minpoll:": 6, 
          "panko::keystone::auth::public_url": "http://192.168.24.7:8977", 
          "tripleo::profile::base::neutron::server::l3_ha_override": "", 
          "swift::proxy::versioned_writes::allow_versioned_writes": true, 
          "tripleo::profile::base::database::mysql::client_bind_address": "192.168.24.12", 
          "swift::keystone::auth::tenant": "service", 
          "tripleo::stunnel::manage_service": false, 
          "nova::db::mysql_placement::user": "nova_placement", 
          "gnocchi::metricd::workers": "%{::os_workers}", 
          "neutron::agents::l3::debug": false, 
          "cinder::rabbit_password": "HtKmbZvPhP8ThyFQxb3PkTKsC", 
          "rabbitmq::service_manage": false, 
          "nova::api::neutron_metadata_proxy_shared_secret": "B6PVNE42RgMZznXdCEW3wZxWV", 
          "nova::wsgi::apache_api::bind_host": "192.168.24.12", 
          "nova::keystone::auth::internal_url": "http://192.168.24.7:8774/v2.1", 
          "nova::cinder_catalog_info": "volumev3:cinderv3:internalURL", 
          "rabbitmq::package_source": "undef", 
          "nova::vncproxy::common::vncproxy_host": "192.168.24.7", 
          "gnocchi::api::enable_proxy_headers_parsing": true, 
          "glance::notify::rabbitmq::rabbit_port": 5672, 
          "tripleo::profile::base::swift::ringbuilder::raw_disk_prefix": "r1z1-", 
          "nova::db::database_db_max_retries": -1, 
          "panko::wsgi::apache::servername": "%{hiera('fqdn_internal_api')}", 
          "tripleo::fencing::config": {}, 
          "cinder::keystone::authtoken::user_domain_name": "Default", 
          "nova::vncproxy::common::vncproxy_protocol": "http", 
          "nova::network::neutron::neutron_auth_type": "v3password", 
          "nova::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "gnocchi::keystone::auth::password": "R6Yn6NmnpN7nAfGMJfmmcsCRZ", 
          "tripleo::haproxy::haproxy_stats_password": "wqmfMtzJ9CYwzQ6K7UjYmBJEX", 
          "apache_remote_proxy_ips_network": "192.168.24.12/24", 
          "tripleo::profile::pacemaker::rabbitmq_bundle::rabbitmq_docker_image": "docker.io/tripleomaster/centos-binary-rabbitmq:pcmklatest", 
          "rabbitmq::erlang_cookie": "9tRJ9nThdZgVnnwhk8A4", 
          "heat::keystone::domain::domain_password": "T8R3yEYqqrjZRPwrZq7VXE6yU", 
          "horizon::customization_module": "", 
          "tripleo.keystone.firewall_rules": {
            "111 keystone": {
              "dport": [
                5000, 
                13000, 
                "35357"
              ]
            }
          }, 
          "neutron::plugins::ml2::type_drivers": [
            "vxlan", 
            "vlan", 
            "flat", 
            "gre"
          ], 
          "ceilometer::host": "%{::fqdn}", 
          "gnocchi::wsgi::apache::servername": "%{hiera('fqdn_internal_api')}", 
          "cinder::ceilometer::notification_driver": "messagingv2", 
          "tripleo::packages::enable_install": false, 
          "nova::db::database_max_retries": -1, 
          "keystone::endpoint::admin_url": "http://192.168.24.7:35357", 
          "tripleo::profile::base::cinder::volume::nfs::cinder_nfs_mount_options": "", 
          "nova::db::mysql_placement::dbname": "nova_placement", 
          "horizon::django_debug": false, 
          "redis::managed_by_cluster_manager": true, 
          "gnocchi::keystone::auth::public_url": "http://192.168.24.7:8041", 
          "swift::storage::all::object_server_workers": "auto", 
          "ceilometer::dispatcher::gnocchi::url": "http://192.168.24.7:8041", 
          "snmp::agentaddress": [
            "udp:161", 
            "udp6:[::1]:161"
          ], 
          "panko::expirer::weekday": "*", 
          "keystone::db::database_db_max_retries": -1, 
          "rabbitmq::interface": "192.168.24.12", 
          "aodh::api::service_name": "httpd", 
          "cinder::db::database_db_max_retries": -1, 
          "tripleo::profile::base::cinder::volume::cinder_enable_nfs_backend": false, 
          "keystone::wsgi::apache::admin_port": "35357", 
          "tripleo::profile::pacemaker::database::redis_bundle::tls_proxy_fqdn": "%{hiera('fqdn_internal_api')}", 
          "gnocchi::statsd::user_id": "27c0d3f8-e7ee-42f0-8317-72237d1c5ae3", 
          "nova::vncproxy::host": "192.168.24.12", 
          "mysql_bind_host": "192.168.24.12", 
          "nova::scheduler::filter::scheduler_max_attempts": 3, 
          "mysql_clustercheck_password": "Dczkk2AR6qhPKbBWAMwVMUQbC", 
          "pacemaker::corosync::settle_tries": 360, 
          "tripleo.memcached.firewall_rules": {
            "121 memcached": {
              "dport": 11211
            }
          }, 
          "tripleo::profile::base::lvm::enable_udev": false, 
          "rabbitmq_config_variables": {
            "queue_master_locator": "<<\"min-masters\">>", 
            "loopback_users": "[]", 
            "cluster_partition_handling": "ignore"
          }, 
          "aodh::db::mysql::user": "aodh", 
          "glance::backend::rbd::rbd_store_pool": "images", 
          "nova::api_database_connection": "mysql+pymysql://nova_api:ePtbUHubCqvanb7c8da8AAupz@192.168.24.7/nova_api?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "ceilometer::keystone::authtoken::password": "FtXc7xgX8w2EJbXcCe3psFzv4", 
          "keystone::public_bind_host": "%{hiera('fqdn_internal_api')}", 
          "tripleo::profile::base::database::mysql::generate_dropin_file_limit": true, 
          "tripleo.mysql.firewall_rules": {
            "104 mysql galera-bundle": {
              "dport": [
                873, 
                3123, 
                3306, 
                4444, 
                4567, 
                4568, 
                9200
              ]
            }
          }, 
          "heat::policy::policies": {}, 
          "nova::network::neutron::neutron_password": "22PeyXGFu7qJevbd3VtKnTeh3", 
          "tripleo.nova_placement.firewall_rules": {
            "138 nova_placement": {
              "dport": [
                8778, 
                13778
              ]
            }
          }, 
          "keystone_enable_db_purge": true, 
          "panko::expirer::month": "*", 
          "ceilometer::keystone::auth::public_url": "http://192.168.24.7:8777", 
          "nova::cron::archive_deleted_rows::hour": "0", 
          "neutron::db::mysql::password": "22PeyXGFu7qJevbd3VtKnTeh3", 
          "rabbitmq_kernel_variables": {
            "inet_dist_listen_min": "25672", 
            "net_ticktime": 15, 
            "inet_dist_listen_max": "25672"
          }, 
          "tripleo::profile::base::swift::ringbuilder::min_part_hours": 1, 
          "tripleo::profile::base::neutron::server::tls_proxy_bind_ip": "192.168.24.12", 
          "keystone::debug": false, 
          "neutron::agents::metadata::auth_url": "http://192.168.24.7:5000", 
          "panko::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "gnocchi::statsd::flush_delay": 10, 
          "ntp::maxpoll:": 10, 
          "glance::api::bind_port": "9292", 
          "heat::db::mysql::dbname": "heat", 
          "swift::proxy::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "swift::proxy::authtoken::auth_url": "http://192.168.24.7:5000", 
          "keystone::admin_port": "35357", 
          "neutron::agents::ml2::ovs::local_ip": "192.168.24.12", 
          "tripleo.redis.firewall_rules": {
            "108 redis-bundle": {
              "dport": [
                3124, 
                6379, 
                26379
              ]
            }
          }, 
          "nova::keystone::authtoken::password": "ePtbUHubCqvanb7c8da8AAupz", 
          "mysql::server::root_password": "RazNVvJAyj", 
          "glance::api::os_region_name": "regionOne", 
          "heat::rabbit_port": 5672, 
          "tripleo::haproxy::ca_bundle": "/etc/ipa/ca.crt", 
          "panko::keystone::authtoken::project_domain_name": "Default", 
          "ceilometer::rabbit_port": 5672, 
          "tripleo::profile::base::cinder::volume::nfs::cinder_nas_secure_file_operations": "False", 
          "memcached::verbosity": "v", 
          "neutron::keystone::authtoken::password": "22PeyXGFu7qJevbd3VtKnTeh3", 
          "glance::backend::swift::swift_store_auth_address": "http://192.168.24.7:5000/v3", 
          "mysql::server::package_name": "mariadb-galera-server", 
          "tripleo::profile::base::cinder::volume::cinder_enable_iscsi_backend": true, 
          "tripleo::profile::pacemaker::database::redis_bundle::tls_proxy_port": 6379, 
          "timezone::timezone": "UTC", 
          "gnocchi::storage::swift::swift_endpoint_type": "internalURL", 
          "mysql_max_connections": 4096, 
          "heat::keystone::auth_cfn::admin_url": "http://192.168.24.7:8000/v1", 
          "ceilometer::db::mysql::allowed_hosts": [
            "%", 
            "%{hiera('mysql_bind_host')}"
          ], 
          "keystone::db::mysql::host": "192.168.24.7", 
          "aodh::db::database_connection": "mysql+pymysql://aodh:zpRBy7zcejYfAbksu9YRVMfhf@192.168.24.7/aodh?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "aodh::api::enable_proxy_headers_parsing": true, 
          "panko::keystone::authtoken::auth_url": "http://192.168.24.7:5000", 
          "tripleo.cinder_api.firewall_rules": {
            "119 cinder": {
              "dport": [
                8776, 
                13776
              ]
            }
          }, 
          "heat::heat_keystone_clients_url": "http://192.168.24.7:5000", 
          "panko::keystone::authtoken::auth_uri": "http://192.168.24.7:5000", 
          "ceilometer_auth_enabled": true, 
          "keystone::db::database_max_retries": -1, 
          "tripleo::profile::base::keystone::heat_admin_password": "T8R3yEYqqrjZRPwrZq7VXE6yU", 
          "swift::keystone::auth::admin_url": "http://192.168.24.7:8080", 
          "heat::database_connection": "mysql+pymysql://heat:HBqZMnZMW7ZhRJhEh3kqBT6BT@192.168.24.7/heat?read_default_group=tripleo&read_default_file=/etc/my.cnf.d/tripleo.cnf", 
          "nova::notify_on_state_change": "vm_and_task_state", 
          "cinder::rabbit_userid": "guest", 
          "redis::sentinel::notification_script": "/usr/local/bin/redis-notifications.sh", 
          "nova::api::metadata_listen": "192.168.24.12", 
          "gnocchi::wsgi::apache::ssl": false, 
          "tripleo::stunnel::foreground": "yes", 
          "swift::storage::all::outgoing_chmod": "Du=rwx,g=rx,o=rx,Fu=rw,g=r,o=r", 
          "tripleo::profile::base::swift::ringbuilder::raw_disks": [
            ":%PORT%/d1"
          ], 
          "keystone::roles::admin::admin_tenant": "admin", 
          "nova::db::sync::db_sync_timeout": 300, 
          "apache::mod::remoteip::proxy_ips": [
            "%{hiera('apache_remote_proxy_ips_network')}"
          ], 
          "redis::sentinel::sentinel_bind": "192.168.24.12", 
          "nova::network::neutron::neutron_username": "neutron", 
          "swift::keystone::auth::public_url_s3": "http://192.168.24.7:8080", 
          "tripleo::profile::base::neutron::server::tls_proxy_port": "9696", 
          "ceilometer::db::mysql::host": "192.168.24.7", 
          "heat::cron::purge_deleted::weekday": "*"
        }, 
        "controller_extraconfig": {}, 
        "service_names": {
          "sensu::subscriptions": [
            "overcloud-pacemaker"
          ], 
          "service_names": [
            "aodh_api", 
            "aodh_evaluator", 
            "aodh_listener", 
            "aodh_notifier", 
            "ca_certs", 
            "ceilometer_api_disabled", 
            "ceilometer_collector_disabled", 
            "ceilometer_expirer_disabled", 
            "ceilometer_agent_central", 
            "ceilometer_agent_notification", 
            "cinder_api", 
            "cinder_scheduler", 
            "cinder_volume", 
            "clustercheck", 
            "docker", 
            "glance_api", 
            "gnocchi_api", 
            "gnocchi_metricd", 
            "gnocchi_statsd", 
            "haproxy", 
            "heat_api", 
            "heat_api_cfn", 
            "heat_engine", 
            "horizon", 
            "iscsid", 
            "kernel", 
            "keystone", 
            "memcached", 
            "mongodb_disabled", 
            "mysql", 
            "mysql_client", 
            "neutron_api", 
            "neutron_plugin_ml2", 
            "neutron_dhcp", 
            "neutron_l3", 
            "neutron_metadata", 
            "neutron_ovs_agent", 
            "nova_api", 
            "nova_conductor", 
            "nova_consoleauth", 
            "nova_metadata", 
            "nova_placement", 
            "nova_scheduler", 
            "nova_vnc_proxy", 
            "ntp", 
            "logrotate_crond", 
            "pacemaker", 
            "panko_api", 
            "rabbitmq", 
            "redis", 
            "snmp", 
            "sshd", 
            "swift_proxy", 
            "swift_ringbuilder", 
            "swift_storage", 
            "timezone", 
            "tripleo_firewall", 
            "tripleo_packages", 
            "tuned"
          ]
        }, 
        "controller": {
          "tripleo::packages::enable_upgrade": false, 
          "fqdn_tenant": "overcloud-controller-1.tenant.localdomain", 
          "tripleo::profile::base::logging::fluentd::fluentd_sources": [], 
          "fqdn_internal_api": "overcloud-controller-1.internalapi.localdomain", 
          "fqdn_storage_mgmt": "overcloud-controller-1.storagemgmt.localdomain", 
          "fqdn_management": "overcloud-controller-1.management.localdomain", 
          "fqdn_external": "overcloud-controller-1.external.localdomain", 
          "tripleo::clouddomain": "localdomain", 
          "fqdn_storage": "overcloud-controller-1.storage.localdomain", 
          "fqdn_canonical": "overcloud-controller-1.localdomain", 
          "fqdn_ctlplane": "overcloud-controller-1.ctlplane.localdomain", 
          "tripleo::profile::base::logging::fluentd::fluentd_groups": [
            "root"
          ]
        }
      }, 
      "merge_behavior": "deeper"
    }
  creation_time: "2018-03-09T19:45:58Z"
  deployment_name: ControllerDeployment
  group: hiera
  id: 0a22bdd6-b4c7-4ed3-9e39-6a7293c0a05c
  inputs:
    - name: enable_package_upgrade
      description: None
      type: String
      value: |-
        False
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-Controller-xnl34i5ufw7g-1-lyve2t6wza3v-ControllerDeployment-rncs3nacccbj/1e580f90-8af1-415a-bfd6-4913627f95de
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:

SshHostPubKeyDeployment:
  config: |
    #!/bin/sh -x
    test -e '/etc/ssh/ssh_host_rsa_key.pub' && cat /etc/ssh/ssh_host_rsa_key.pub > $heat_outputs_path.rsa
    test -e '/etc/ssh/ssh_host_ecdsa_key.pub' && cat /etc/ssh/ssh_host_ecdsa_key.pub > $heat_outputs_path.ecdsa
    test -e '/etc/ssh/ssh_host_ed25519_key.pub' && cat /etc/ssh/ssh_host_ed25519_key.pub > $heat_outputs_path.ed25519
  creation_time: "2018-03-09T19:46:14Z"
  deployment_name: SshHostPubKeyDeployment
  group: script
  id: 156eddaa-ce1a-46c8-a146-2d37d0c5bb32
  inputs:
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-Controller-xnl34i5ufw7g-1-lyve2t6wza3v-SshHostPubKey-ny2taytojn5l-SshHostPubKeyDeployment-trr4zbe3ueji/2f46a1ef-5b76-4721-b476-f57b71395570
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:
    - name: rsa
      description: 
      type: String
    - name: ecdsa
      description: 
      type: String
    - name: ed25519
      description: 
      type: String

ControllerHostsDeployment:
  config: |
    #!/bin/bash
    set -eux
    set -o pipefail
    
    write_entries() {
        local file="$1"
        local entries="$2"
    
        # Don't do anything if the file isn't there
        if [ ! -f "$file" ]; then
            return
        fi
    
        if grep -q "^# HEAT_HOSTS_START" "$file"; then
            temp=$(mktemp)
            (
            sed '/^# HEAT_HOSTS_START/,$d' "$file"
            echo -ne "\n# HEAT_HOSTS_START - Do not edit manually within this section!\n"
            echo "$entries"
            echo -ne "# HEAT_HOSTS_END\n\n"
            sed '1,/^# HEAT_HOSTS_END/d' "$file"
            ) > "$temp"
            echo "INFO: Updating hosts file $file, check below for changes"
            diff "$file" "$temp" || true
            cat "$temp" > "$file"
        else
            echo -ne "\n# HEAT_HOSTS_START - Do not edit manually within this section!\n" >> "$file"
            echo "$entries" >> "$file"
            echo -ne "# HEAT_HOSTS_END\n\n" >> "$file"
        fi
    
    }
    
    if [ ! -z "$hosts" ]; then
        for tmpl in /etc/cloud/templates/hosts.*.tmpl ; do
            write_entries "$tmpl" "$hosts"
        done
        write_entries "/etc/hosts" "$hosts"
    else
        echo "No hosts in Heat, nothing written."
    fi
  creation_time: "2018-03-09T19:46:40Z"
  deployment_name: ControllerHostsDeployment
  group: script
  id: e491fc51-6068-4350-b484-fa30603f0e50
  inputs:
    - name: hosts
      description: 
      type: String
      value: |-
        192.168.24.7  overcloud.ctlplane.localdomain
        192.168.24.7  overcloud.storage.localdomain
        192.168.24.7  overcloud.storagemgmt.localdomain
        192.168.24.7  overcloud.internalapi.localdomain
        192.168.24.7  overcloud.localdomain
        192.168.24.16 overcloud-controller-0.localdomain overcloud-controller-0
        192.168.24.16 overcloud-controller-0.storage.localdomain overcloud-controller-0.storage
        192.168.24.16 overcloud-controller-0.storagemgmt.localdomain overcloud-controller-0.storagemgmt
        192.168.24.16 overcloud-controller-0.internalapi.localdomain overcloud-controller-0.internalapi
        192.168.24.16 overcloud-controller-0.tenant.localdomain overcloud-controller-0.tenant
        192.168.24.16 overcloud-controller-0.external.localdomain overcloud-controller-0.external
        192.168.24.16 overcloud-controller-0.management.localdomain overcloud-controller-0.management
        192.168.24.16 overcloud-controller-0.ctlplane.localdomain overcloud-controller-0.ctlplane
        192.168.24.12 overcloud-controller-1.localdomain overcloud-controller-1
        192.168.24.12 overcloud-controller-1.storage.localdomain overcloud-controller-1.storage
        192.168.24.12 overcloud-controller-1.storagemgmt.localdomain overcloud-controller-1.storagemgmt
        192.168.24.12 overcloud-controller-1.internalapi.localdomain overcloud-controller-1.internalapi
        192.168.24.12 overcloud-controller-1.tenant.localdomain overcloud-controller-1.tenant
        192.168.24.12 overcloud-controller-1.external.localdomain overcloud-controller-1.external
        192.168.24.12 overcloud-controller-1.management.localdomain overcloud-controller-1.management
        192.168.24.12 overcloud-controller-1.ctlplane.localdomain overcloud-controller-1.ctlplane
        192.168.24.18 overcloud-controller-2.localdomain overcloud-controller-2
        192.168.24.18 overcloud-controller-2.storage.localdomain overcloud-controller-2.storage
        192.168.24.18 overcloud-controller-2.storagemgmt.localdomain overcloud-controller-2.storagemgmt
        192.168.24.18 overcloud-controller-2.internalapi.localdomain overcloud-controller-2.internalapi
        192.168.24.18 overcloud-controller-2.tenant.localdomain overcloud-controller-2.tenant
        192.168.24.18 overcloud-controller-2.external.localdomain overcloud-controller-2.external
        192.168.24.18 overcloud-controller-2.management.localdomain overcloud-controller-2.management
        192.168.24.18 overcloud-controller-2.ctlplane.localdomain overcloud-controller-2.ctlplane
        
        192.168.24.13 overcloud-novacompute-0.localdomain overcloud-novacompute-0
        192.168.24.13 overcloud-novacompute-0.storage.localdomain overcloud-novacompute-0.storage
        192.168.24.13 overcloud-novacompute-0.storagemgmt.localdomain overcloud-novacompute-0.storagemgmt
        192.168.24.13 overcloud-novacompute-0.internalapi.localdomain overcloud-novacompute-0.internalapi
        192.168.24.13 overcloud-novacompute-0.tenant.localdomain overcloud-novacompute-0.tenant
        192.168.24.13 overcloud-novacompute-0.external.localdomain overcloud-novacompute-0.external
        192.168.24.13 overcloud-novacompute-0.management.localdomain overcloud-novacompute-0.management
        192.168.24.13 overcloud-novacompute-0.ctlplane.localdomain overcloud-novacompute-0.ctlplane
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-ControllerHostsDeployment-m3ifnbalzb4k-1-rkiko743ovcd/020ee433-ad44-4845-9dce-2fa84d507874
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:

ControllerSshKnownHostsDeployment:
  config: |
    #!/bin/bash
    set -eux
    set -o pipefail
    
    echo "Creating ssh known hosts file"
    
    if [ ! -z "${known_hosts}" ]; then
      echo "${known_hosts}"
      echo -ne "${known_hosts}" > /etc/ssh/ssh_known_hosts
      chmod 0644 /etc/ssh/ssh_known_hosts
    else
      rm -f /etc/ssh/ssh_known_hosts
      echo "No ssh known hosts"
    fi
  creation_time: "2018-03-09T19:46:41Z"
  deployment_name: ControllerSshKnownHostsDeployment
  group: script
  id: ecc7f6ee-4021-4a9d-b638-8184884ad43c
  inputs:
    - name: known_hosts
      description: 
      type: String
      value: |-
        192.168.24.16,overcloud-controller-0.localdomain,overcloud-controller-0,192.168.24.16,overcloud-controller-0.storage.localdomain,overcloud-controller-0.storage,192.168.24.16,overcloud-controller-0.storagemgmt.localdomain,overcloud-controller-0.storagemgmt,192.168.24.16,overcloud-controller-0.internalapi.localdomain,overcloud-controller-0.internalapi,192.168.24.16,overcloud-controller-0.tenant.localdomain,overcloud-controller-0.tenant,192.168.24.16,overcloud-controller-0.external.localdomain,overcloud-controller-0.external,192.168.24.16,overcloud-controller-0.management.localdomain,overcloud-controller-0.management,192.168.24.16,overcloud-controller-0.ctlplane.localdomain,overcloud-controller-0.ctlplane ecdsa192.168.24.12,overcloud-controller-1.localdomain,overcloud-controller-1,192.168.24.12,overcloud-controller-1.storage.localdomain,overcloud-controller-1.storage,192.168.24.12,overcloud-controller-1.storagemgmt.localdomain,overcloud-controller-1.storagemgmt,192.168.24.12,overcloud-controller-1.internalapi.localdomain,overcloud-controller-1.internalapi,192.168.24.12,overcloud-controller-1.tenant.localdomain,overcloud-controller-1.tenant,192.168.24.12,overcloud-controller-1.external.localdomain,overcloud-controller-1.external,192.168.24.12,overcloud-controller-1.management.localdomain,overcloud-controller-1.management,192.168.24.12,overcloud-controller-1.ctlplane.localdomain,overcloud-controller-1.ctlplane ecdsa192.168.24.18,overcloud-controller-2.localdomain,overcloud-controller-2,192.168.24.18,overcloud-controller-2.storage.localdomain,overcloud-controller-2.storage,192.168.24.18,overcloud-controller-2.storagemgmt.localdomain,overcloud-controller-2.storagemgmt,192.168.24.18,overcloud-controller-2.internalapi.localdomain,overcloud-controller-2.internalapi,192.168.24.18,overcloud-controller-2.tenant.localdomain,overcloud-controller-2.tenant,192.168.24.18,overcloud-controller-2.external.localdomain,overcloud-controller-2.external,192.168.24.18,overcloud-controller-2.management.localdomain,overcloud-controller-2.management,192.168.24.18,overcloud-controller-2.ctlplane.localdomain,overcloud-controller-2.ctlplane ecdsa192.168.24.13,overcloud-novacompute-0.localdomain,overcloud-novacompute-0,192.168.24.13,overcloud-novacompute-0.storage.localdomain,overcloud-novacompute-0.storage,192.168.24.13,overcloud-novacompute-0.storagemgmt.localdomain,overcloud-novacompute-0.storagemgmt,192.168.24.13,overcloud-novacompute-0.internalapi.localdomain,overcloud-novacompute-0.internalapi,192.168.24.13,overcloud-novacompute-0.tenant.localdomain,overcloud-novacompute-0.tenant,192.168.24.13,overcloud-novacompute-0.external.localdomain,overcloud-novacompute-0.external,192.168.24.13,overcloud-novacompute-0.management.localdomain,overcloud-novacompute-0.management,192.168.24.13,overcloud-novacompute-0.ctlplane.localdomain,overcloud-novacompute-0.ctlplane ecdsa
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-ControllerSshKnownHostsDeployment-afrrx6kau2fd-1-bivvvpjqxdcw/f44bb5a6-30e3-4382-9ac1-6dbe340dd49a
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:

ControllerAllNodesDeployment:
  config:
    {
      "datafiles": {
        "all_nodes": {
          "heat_api_enabled": "true", 
          "cinder_volume_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "gnocchi_api_network": "internal_api", 
          "mongodb_disabled_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "swift_proxy_short_bootstrap_node_name": "overcloud-controller-0", 
          "gnocchi_metricd_short_bootstrap_node_name": "overcloud-controller-0", 
          "cinder_volume_enabled": "true", 
          "ca_certs_enabled": "true", 
          "tuned_enabled": "true", 
          "nova_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "swift_storage_node_names": [
            "overcloud-controller-0.storagemgmt.localdomain", 
            "overcloud-controller-1.storagemgmt.localdomain", 
            "overcloud-controller-2.storagemgmt.localdomain"
          ], 
          "tripleo_packages_short_bootstrap_node_name": "overcloud-controller-0", 
          "aodh_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "cinder_scheduler_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "nova_metadata_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "keystone_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "swift_proxy_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "aodh_api_network": "internal_api", 
          "gnocchi_metricd_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "nova_metadata_enabled": "true", 
          "nova_migration_target_short_node_names": [
            "overcloud-novacompute-0"
          ], 
          "neutron_plugin_ml2_enabled": "true", 
          "clustercheck_enabled": "true", 
          "iscsid_enabled": "true", 
          "redis_enabled": "true", 
          "ntp_enabled": "true", 
          "panko_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "neutron_ovs_agent_enabled": "true", 
          "cellv2_discovery_hosts": "overcloud-novacompute-0.localdomain", 
          "ceilometer_agent_central_enabled": "true", 
          "heat_api_network": "internal_api", 
          "nova_vnc_proxy_short_bootstrap_node_name": "overcloud-controller-0", 
          "tripleo_packages_enabled": "true", 
          "haproxy_enabled": "true", 
          "timezone_enabled": "true", 
          "keystone_admin_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "mysql_client_short_bootstrap_node_name": "overcloud-controller-0", 
          "cinder_scheduler_short_bootstrap_node_name": "overcloud-controller-0", 
          "neutron_dhcp_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_compute_enabled": "true", 
          "nova_scheduler_short_bootstrap_node_name": "overcloud-controller-0", 
          "ntp_short_bootstrap_node_name": "overcloud-controller-0", 
          "memcached_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "gnocchi_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "neutron_ovs_agent_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "nova_libvirt_short_node_names": [
            "overcloud-novacompute-0"
          ], 
          "docker_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "mongodb_disabled_enabled": "true", 
          "ceilometer_agent_compute_short_bootstrap_node_name": "overcloud-novacompute-0", 
          "nova_placement_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "nova_conductor_enabled": "true", 
          "neutron_metadata_enabled": "true", 
          "nova_metadata_short_bootstrap_node_name": "overcloud-controller-0", 
          "sshd_enabled": "true", 
          "ceilometer_api_disabled_enabled": "true", 
          "swift_storage_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "nova_vnc_proxy_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "nova_placement_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "cinder_api_network": "internal_api", 
          "horizon_enabled": "true", 
          "cinder_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "heat_api_cfn_enabled": "true", 
          "aodh_listener_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "timezone_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "nova_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "nova_consoleauth_enabled": "true", 
          "heat_api_cfn_short_bootstrap_node_name": "overcloud-controller-0", 
          "heat_engine_enabled": "true", 
          "swift_ringbuilder_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_placement_enabled": "true", 
          "cinder_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "ca_certs_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "swift_storage_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "keystone_admin_api_network": "ctlplane", 
          "heat_engine_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "nova_consoleauth_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "panko_api_enabled": "true", 
          "nova_api_enabled": "true", 
          "keystone_public_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "swift_storage_enabled": "true", 
          "gnocchi_api_enabled": "true", 
          "nova_vnc_proxy_enabled": "true", 
          "heat_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_libvirt_node_ips": [
            "192.168.24.13"
          ], 
          "heat_api_cfn_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "kernel_enabled": "true", 
          "neutron_api_network": "internal_api", 
          "nova_metadata_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "aodh_notifier_enabled": "true", 
          "neutron_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "keystone_admin_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "glance_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "memcached_network": "internal_api", 
          "neutron_dhcp_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "gnocchi_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "cinder_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "aodh_notifier_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "ceilometer_agent_notification_short_bootstrap_node_name": "overcloud-controller-0", 
          "mysql_network": "internal_api", 
          "glance_api_enabled": "true", 
          "cinder_scheduler_enabled": "true", 
          "pacemaker_enabled": "true", 
          "nova_vnc_proxy_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "nova_metadata_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "kernel_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "ntp_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "keystone_public_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "iscsid_short_bootstrap_node_name": "overcloud-controller-0", 
          "ceilometer_collector_disabled_short_bootstrap_node_name": "overcloud-controller-0", 
          "neutron_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "neutron_metadata_short_bootstrap_node_name": "overcloud-controller-0", 
          "tripleo_firewall_short_bootstrap_node_name": "overcloud-controller-0", 
          "aodh_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "snmp_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "clustercheck_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "nova_conductor_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "update_identifier": "", 
          "neutron_metadata_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "heat_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "nova_placement_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "heat_api_cfn_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "horizon_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "cinder_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "keystone_enabled": "true", 
          "tripleo_firewall_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "redis_short_bootstrap_node_name": "overcloud-controller-0", 
          "mysql_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "tuned_short_bootstrap_node_name": "overcloud-controller-0", 
          "aodh_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "logrotate_crond_short_bootstrap_node_name": "overcloud-controller-0", 
          "glance_api_network": "internal_api", 
          "memcached_enabled": "true", 
          "redis_network": "internal_api", 
          "mysql_enabled": "true", 
          "ca_certs_short_bootstrap_node_name": "overcloud-controller-0", 
          "aodh_listener_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_vnc_proxy_network": "internal_api", 
          "keystone_admin_api_node_names": [
            "overcloud-controller-0.ctlplane.localdomain", 
            "overcloud-controller-1.ctlplane.localdomain", 
            "overcloud-controller-2.ctlplane.localdomain"
          ], 
          "aodh_evaluator_enabled": "true", 
          "ceilometer_agent_compute_enabled": "true", 
          "tripleo_firewall_enabled": "true", 
          "rabbitmq_short_bootstrap_node_name": "overcloud-controller-0", 
          "mysql_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "sshd_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "heat_api_cfn_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "nova_api_network": "internal_api", 
          "rabbitmq_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "nova_migration_target_short_bootstrap_node_name": "overcloud-novacompute-0", 
          "swift_proxy_enabled": "true", 
          "swift_storage_network": "storage_mgmt", 
          "snmp_enabled": "true", 
          "logrotate_crond_enabled": "true", 
          "tripleo_packages_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "tuned_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "ceilometer_agent_notification_enabled": "true", 
          "enabled_services": [
            "aodh_api", 
            "aodh_evaluator", 
            "aodh_listener", 
            "aodh_notifier", 
            "ca_certs", 
            "ceilometer_api_disabled", 
            "ceilometer_collector_disabled", 
            "ceilometer_expirer_disabled", 
            "ceilometer_agent_central", 
            "ceilometer_agent_notification", 
            "cinder_api", 
            "cinder_scheduler", 
            "cinder_volume", 
            "clustercheck", 
            "docker", 
            "glance_api", 
            "gnocchi_api", 
            "gnocchi_metricd", 
            "gnocchi_statsd", 
            "haproxy", 
            "heat_api", 
            "heat_api_cfn", 
            "heat_engine", 
            "horizon", 
            "iscsid", 
            "kernel", 
            "keystone", 
            "memcached", 
            "mongodb_disabled", 
            "mysql", 
            "mysql_client", 
            "neutron_api", 
            "neutron_plugin_ml2", 
            "neutron_dhcp", 
            "neutron_l3", 
            "neutron_metadata", 
            "neutron_ovs_agent", 
            "nova_api", 
            "nova_conductor", 
            "nova_consoleauth", 
            "nova_metadata", 
            "nova_placement", 
            "nova_scheduler", 
            "nova_vnc_proxy", 
            "ntp", 
            "logrotate_crond", 
            "pacemaker", 
            "panko_api", 
            "rabbitmq", 
            "redis", 
            "snmp", 
            "sshd", 
            "swift_proxy", 
            "swift_ringbuilder", 
            "swift_storage", 
            "timezone", 
            "tripleo_firewall", 
            "tripleo_packages", 
            "tuned", 
            "ceilometer_agent_compute", 
            "nova_compute", 
            "nova_libvirt", 
            "nova_migration_target"
          ], 
          "glance_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "stack_update_type": "", 
          "iscsid_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "horizon_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "gnocchi_statsd_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "mongodb_disabled_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_libvirt_enabled": "true", 
          "redis_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "gnocchi_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_migration_target_enabled": "true", 
          "ceilometer_agent_central_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "haproxy_short_bootstrap_node_name": "overcloud-controller-0", 
          "clustercheck_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_libvirt_network": "internal_api", 
          "memcached_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "swift_proxy_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "snmp_short_bootstrap_node_name": "overcloud-controller-0", 
          "panko_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "mysql_client_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "neutron_plugin_ml2_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "aodh_listener_enabled": "true", 
          "panko_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_placement_network": "internal_api", 
          "nova_metadata_network": "internal_api", 
          "nova_placement_short_bootstrap_node_name": "overcloud-controller-0", 
          "keystone_public_api_network": "internal_api", 
          "mysql_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "nova_libvirt_short_bootstrap_node_name": "overcloud-novacompute-0", 
          "pacemaker_short_bootstrap_node_name": "overcloud-controller-0", 
          "gnocchi_statsd_short_bootstrap_node_name": "overcloud-controller-0", 
          "logrotate_crond_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2", 
            "overcloud-novacompute-0"
          ], 
          "keystone_public_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "gnocchi_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "docker_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_scheduler_enabled": "true", 
          "neutron_l3_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "nova_vnc_proxy_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "nova_libvirt_node_names": [
            "overcloud-novacompute-0.internalapi.localdomain"
          ], 
          "heat_api_cfn_network": "internal_api", 
          "neutron_l3_enabled": "true", 
          "neutron_ovs_agent_short_bootstrap_node_name": "overcloud-controller-0", 
          "rabbitmq_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "redis_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "cinder_volume_short_bootstrap_node_name": "overcloud-controller-0", 
          "mysql_short_bootstrap_node_name": "overcloud-controller-0", 
          "redis_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "neutron_plugin_ml2_short_bootstrap_node_name": "overcloud-controller-0", 
          "ceilometer_agent_compute_short_node_names": [
            "overcloud-novacompute-0"
          ], 
          "swift_proxy_node_names": [
            "overcloud-controller-0.storage.localdomain", 
            "overcloud-controller-1.storage.localdomain", 
            "overcloud-controller-2.storage.localdomain"
          ], 
          "pacemaker_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "rabbitmq_network": "internal_api", 
          "swift_storage_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "sshd_short_bootstrap_node_name": "overcloud-controller-0", 
          "rabbitmq_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "glance_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "neutron_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "deploy_identifier": "1520623270", 
          "horizon_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "swift_ringbuilder_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "neutron_l3_short_bootstrap_node_name": "overcloud-controller-0", 
          "memcached_short_bootstrap_node_name": "overcloud-controller-0", 
          "keystone_public_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "heat_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "ceilometer_expirer_disabled_enabled": "true", 
          "swift_proxy_network": "storage", 
          "nova_conductor_short_bootstrap_node_name": "overcloud-controller-0", 
          "ceilometer_collector_disabled_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "ceilometer_api_disabled_short_bootstrap_node_name": "overcloud-controller-0", 
          "aodh_notifier_short_bootstrap_node_name": "overcloud-controller-0", 
          "ceilometer_agent_notification_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "rabbitmq_enabled": "true", 
          "controller_node_ips": "192.168.24.16,192.168.24.12,192.168.24.18", 
          "nova_compute_short_node_names": [
            "overcloud-novacompute-0"
          ], 
          "ceilometer_agent_central_short_bootstrap_node_name": "overcloud-controller-0", 
          "ceilometer_expirer_disabled_short_bootstrap_node_name": "overcloud-controller-0", 
          "aodh_api_enabled": "true", 
          "ceilometer_expirer_disabled_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "heat_engine_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_consoleauth_short_bootstrap_node_name": "overcloud-controller-0", 
          "memcached_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "haproxy_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "horizon_network": "internal_api", 
          "ceilometer_collector_disabled_enabled": "true", 
          "neutron_api_enabled": "true", 
          "cinder_api_enabled": "true", 
          "stack_action": "CREATE", 
          "nova_compute_short_bootstrap_node_name": "overcloud-novacompute-0", 
          "aodh_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "neutron_dhcp_enabled": "true", 
          "swift_ringbuilder_enabled": "true", 
          "aodh_evaluator_short_bootstrap_node_name": "overcloud-controller-0", 
          "keystone_short_bootstrap_node_name": "overcloud-controller-0", 
          "nova_scheduler_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "gnocchi_statsd_enabled": "true", 
          "neutron_api_short_bootstrap_node_name": "overcloud-controller-0", 
          "keystone_admin_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "gnocchi_metricd_enabled": "true", 
          "mysql_client_enabled": "true", 
          "panko_api_node_names": [
            "overcloud-controller-0.internalapi.localdomain", 
            "overcloud-controller-1.internalapi.localdomain", 
            "overcloud-controller-2.internalapi.localdomain"
          ], 
          "nova_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "glance_api_node_ips": [
            "192.168.24.16", 
            "192.168.24.12", 
            "192.168.24.18"
          ], 
          "heat_api_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "panko_api_network": "internal_api", 
          "docker_enabled": "true", 
          "controller_node_names": "overcloud-controller-0,overcloud-controller-1,overcloud-controller-2", 
          "kernel_short_bootstrap_node_name": "overcloud-controller-0", 
          "aodh_evaluator_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ], 
          "horizon_short_bootstrap_node_name": "overcloud-controller-0", 
          "timezone_short_bootstrap_node_name": "overcloud-controller-0", 
          "ceilometer_api_disabled_short_node_names": [
            "overcloud-controller-0", 
            "overcloud-controller-1", 
            "overcloud-controller-2"
          ]
        }, 
        "vip_data": {
          "internal_api_virtual_ip": "192.168.24.7", 
          "cloud_name_storage": "overcloud.storage.localdomain", 
          "redis_vip": "192.168.24.9", 
          "rabbitmq_vip": "192.168.24.7", 
          "nova_libvirt_vip": "192.168.24.7", 
          "public_virtual_ip": "192.168.24.7", 
          "memcached_vip": "192.168.24.7", 
          "neutron_api_vip": "192.168.24.7", 
          "aodh_api_vip": "192.168.24.7", 
          "horizon_vip": "192.168.24.7", 
          "tripleo::keepalived::redis_virtual_ip": "192.168.24.9", 
          "tripleo::haproxy::controller_virtual_ip": "192.168.24.7", 
          "panko_api_vip": "192.168.24.7", 
          "enable_internal_tls": false, 
          "swift_proxy_vip": "192.168.24.7", 
          "tripleo::keepalived::controller_virtual_ip": "192.168.24.7", 
          "gnocchi_api_vip": "192.168.24.7", 
          "heat_api_cfn_vip": "192.168.24.7", 
          "nova_metadata_vip": "192.168.24.7", 
          "tripleo::keepalived::public_virtual_ip": "192.168.24.7", 
          "tripleo::haproxy::public_virtual_ip": "192.168.24.7", 
          "tripleo::redis_notification::haproxy_monitor_ip": "192.168.24.7", 
          "mysql_vip": "192.168.24.7", 
          "nova_placement_vip": "192.168.24.7", 
          "heat_api_vip": "192.168.24.7", 
          "keystone_public_api_vip": "192.168.24.7", 
          "nova_api_vip": "192.168.24.7", 
          "cloud_name_internal_api": "overcloud.internalapi.localdomain", 
          "glance_api_vip": "192.168.24.7", 
          "controller_virtual_ip": "192.168.24.7", 
          "keystone_admin_api_vip": "192.168.24.7", 
          "cloud_name_external": "overcloud.localdomain", 
          "cloud_name_ctlplane": "overcloud.ctlplane.localdomain", 
          "swift_storage_vip": "192.168.24.7", 
          "cinder_api_vip": "192.168.24.7", 
          "cloud_name_storage_mgmt": "overcloud.storagemgmt.localdomain", 
          "network_virtual_ips": {
            "storage_mgmt": {
              "index": 2, 
              "ip_address": "192.168.24.7"
            }, 
            "storage": {
              "index": 1, 
              "ip_address": "192.168.24.7"
            }, 
            "internal_api": {
              "index": 3, 
              "ip_address": "192.168.24.7"
            }
          }, 
          "nova_vnc_proxy_vip": "192.168.24.7"
        }, 
        "bootstrap_node": {
          "bootstrap_nodeid": "overcloud-controller-0", 
          "bootstrap_nodeid_ip": "192.168.24.16"
        }
      }
    }
  creation_time: "2018-03-09T19:46:53Z"
  deployment_name: ControllerAllNodesDeployment
  group: hiera
  id: c78c74e3-209f-4a04-8d1e-73f74bfa89d8
  inputs:
    - name: bootstrap_nodeid
      description: None
      type: String
      value: |-
        overcloud-controller-0
    - name: bootstrap_nodeid_ip
      description: None
      type: String
      value: |-
        192.168.24.16
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-ControllerAllNodesDeployment-n3sb4cf4zfwn-1-guacvbvlxs5i/d758b6df-1203-4310-9812-432466ad5afc
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:

ControllerAllNodesValidationDeployment:
  config: |
    #!/bin/bash
    set -e
    
    function ping_retry() {
      local IP_ADDR=$1
      local TIMES=${2:-'10'}
      local COUNT=0
      local PING_CMD=ping
      if [[ $IP_ADDR =~ ":" ]]; then
        PING_CMD=ping6
      fi
      until [ $COUNT -ge $TIMES ]; do
        if $PING_CMD -w 10 -c 1 $IP_ADDR &> /dev/null; then
          echo "Ping to $IP_ADDR succeeded."
          return 0
        fi
        echo "Ping to $IP_ADDR failed. Retrying..."
        COUNT=$(($COUNT + 1))
        sleep 60
      done
      return 1
    }
    
    # For each unique remote IP (specified via Heat) we check to
    # see if one of the locally configured networks matches and if so we
    # attempt a ping test the remote network IP.
    function ping_controller_ips() {
      local REMOTE_IPS=$1
      for REMOTE_IP in $(echo $REMOTE_IPS | sed -e "s| |\n|g" | sort -u); do
        if [[ $REMOTE_IP =~ ":" ]]; then
          networks=$(ip -6 r | grep -v default | cut -d " " -f 1 | grep -v "unreachable")
        else
          networks=$(ip r | grep -v default | cut -d " " -f 1)
        fi
        for LOCAL_NETWORK in $networks; do
          in_network=$(python -c "import ipaddress; net=ipaddress.ip_network(unicode('$LOCAL_NETWORK')); addr=ipaddress.ip_address(unicode('$REMOTE_IP')); print(addr in net)")
          if [[ $in_network == "True" ]]; then
            echo "Trying to ping $REMOTE_IP for local network ${LOCAL_NETWORK}."
            set +e
            if ! ping_retry $REMOTE_IP; then
              echo "FAILURE"
              echo "$REMOTE_IP is not pingable. Local Network: $LOCAL_NETWORK" >&2
              exit 1
            fi
            set -e
            echo "SUCCESS"
          fi
        done
      done
    }
    
    # Ping all default gateways. There should only be one
    # if using upstream t-h-t network templates but we test
    # all of them should some manual network config have
    # multiple gateways.
    function ping_default_gateways() {
      DEFAULT_GW=$(ip r | grep ^default | cut -d " " -f 3)
      set +e
      for GW in $DEFAULT_GW; do
        echo -n "Trying to ping default gateway ${GW}..."
        if ! ping_retry $GW; then
          echo "FAILURE"
          echo "$GW is not pingable."
          exit 1
        fi
      done
      set -e
      echo "SUCCESS"
    }
    
    # Verify the FQDN from the nova/ironic deployment matches
    # FQDN in the heat templates.
    function fqdn_check() {
      HOSTNAME=$(hostname)
      SHORT_NAME=$(hostname -s)
      FQDN_FROM_HOSTS=$(awk '$3 == "'${SHORT_NAME}'"{print $2}' /etc/hosts)
      echo -n "Checking hostname vs /etc/hosts entry..."
      if [[ $HOSTNAME != $FQDN_FROM_HOSTS ]]; then
        echo "FAILURE"
        echo -e "System hostname: ${HOSTNAME}\nEntry from /etc/hosts: ${FQDN_FROM_HOSTS}\n"
        exit 1
      fi
      echo "SUCCESS"
    }
    
    # Verify at least one time source is available.
    function ntp_check() {
      NTP_SERVERS=$(hiera ntp::servers nil |tr -d '[],"')
      if [[ "$NTP_SERVERS" != "nil" ]];then
        echo -n "Testing NTP..."
        NTP_SUCCESS=0
        for NTP_SERVER in $NTP_SERVERS; do
          set +e
          NTPDATE_OUT=$(ntpdate -qud $NTP_SERVER 2>&1)
          NTPDATE_EXIT=$?
          set -e
          if [[ "$NTPDATE_EXIT" == "0" ]];then
            NTP_SUCCESS=1
            break
          else
            NTPDATE_OUT_FULL="$NTPDATE_OUT_FULL $NTPDATE_OUT"
          fi
        done
        if  [[ "$NTP_SUCCESS" == "0" ]];then
          echo "FAILURE"
          echo "$NTPDATE_OUT_FULL"
          exit 1
        fi
        echo "SUCCESS"
      fi
    }
    
    ping_controller_ips "$ping_test_ips"
    ping_default_gateways
    if [[ $validate_fqdn == "True" ]];then
      fqdn_check
    fi
    if [[ $validate_ntp == "True" ]];then
      ntp_check
    fi
  creation_time: "2018-03-09T19:47:04Z"
  deployment_name: ControllerAllNodesValidationDeployment
  group: script
  id: 4402cb15-84aa-4f60-b7da-9d281308f7f0
  inputs:
    - name: ping_test_ips
      description: 
      type: String
      value: |-
        192.168.24.16 192.168.24.16 192.168.24.16 192.168.24.16 192.168.24.16
    - name: validate_fqdn
      description: 
      type: String
      value: |-
        False
    - name: validate_ntp
      description: 
      type: String
      value: |-
        True
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-ControllerAllNodesValidationDeployment-umtgsxweauf2-1-c52l7ca67kue/dba7edef-205d-49f2-87a3-b9cb9ed176c7
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:

ControllerArtifactsDeploy:
  config: |
    #!/bin/bash
    
    TMP_DATA=$(mktemp -d)
    function cleanup {
      rm -Rf "$TMP_DATA"
    }
    trap cleanup EXIT
    
    if [ -n "$artifact_urls" ]; then
      for URL in $(echo $artifact_urls | sed -e "s| |\n|g" | sort -u); do
        curl --globoff -o $TMP_DATA/file_data "$URL"
        if file -b $TMP_DATA/file_data | grep RPM &>/dev/null; then
          mv $TMP_DATA/file_data $TMP_DATA/file_data.rpm
          yum install -y $TMP_DATA/file_data.rpm
          rm $TMP_DATA/file_data.rpm
        elif file -b $TMP_DATA/file_data | grep 'gzip compressed data' &>/dev/null; then
          pushd /
          tar xvzf $TMP_DATA/file_data
          popd
        else
          echo "ERROR: Unsupported file format: $URL"
          exit 1
        fi
        if [ -f $TMP_DATA/file_data ]; then
          rm $TMP_DATA/file_data
        fi
      done
    else
      echo "No artifact_urls was set. Skipping..."
    fi
  creation_time: "2018-03-09T19:47:29Z"
  deployment_name: ControllerArtifactsDeploy
  group: script
  id: 2d042129-bf7e-407d-82b9-ff671b8fd001
  inputs:
    - name: artifact_urls
      description: 
      type: String
      value: |-
        
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-AllNodesDeploySteps-xmid3d3sdvjm-ControllerArtifactsDeploy-7w3h7rcoieji-1-tcxi25pmhmih/c13aee2b-b221-4814-8586-433d8f9aeb54
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {}
  outputs:

ControllerHostPrepDeployment:
  config: |
    [
      {
        "connection": "local", 
        "tasks": [
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/aodh", 
              "/var/log/containers/httpd/aodh-api"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from aodh containers can be found under\n/var/log/containers/aodh and /var/log/containers/httpd/aodh-api.\n", 
              "dest": "/var/log/aodh/readme.txt"
            }, 
            "name": "aodh logs readme"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "/var/log/containers/aodh", 
              "state": "directory"
            }
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "/var/log/containers/ceilometer", 
              "state": "directory"
            }
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from ceilometer containers can be found under\n/var/log/containers/ceilometer.\n", 
              "dest": "/var/log/ceilometer/readme.txt"
            }, 
            "name": "ceilometer logs readme"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/cinder", 
              "/var/log/containers/httpd/cinder-api"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from cinder containers can be found under\n/var/log/containers/cinder and /var/log/containers/httpd/cinder-api.\n", 
              "dest": "/var/log/cinder/readme.txt"
            }, 
            "name": "cinder logs readme"
          }, 
          {
            "name": "create persistent directories", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/cinder"
            ]
          }, 
          {
            "name": "create persistent directories", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/cinder", 
              "/var/lib/cinder"
            ]
          }, 
          {
            "name": "ensure ceph configurations exist", 
            "file": {
              "path": "/etc/ceph", 
              "state": "directory"
            }
          }, 
          {
            "name": "cinder_enable_iscsi_backend fact", 
            "set_fact": {
              "cinder_enable_iscsi_backend": true
            }
          }, 
          {
            "when": "cinder_enable_iscsi_backend", 
            "args": {
              "creates": "/var/lib/cinder/cinder-volumes"
            }, 
            "command": "dd if=/dev/zero of=/var/lib/cinder/cinder-volumes bs=1 count=0 seek=10280M", 
            "name": "cinder create LVM volume group dd"
          }, 
          {
            "shell": "if ! losetup /dev/loop2; then\n  losetup /dev/loop2 /var/lib/cinder/cinder-volumes\nfi\nif ! pvdisplay | grep cinder-volumes; then\n  pvcreate /dev/loop2\nfi\nif ! vgdisplay | grep cinder-volumes; then\n  vgcreate cinder-volumes /dev/loop2\nfi\n", 
            "args": {
              "creates": "/dev/loop2", 
              "executable": "/bin/bash"
            }, 
            "when": "cinder_enable_iscsi_backend", 
            "name": "cinder create LVM volume group"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/glance"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from glance containers can be found under\n/var/log/containers/glance.\n", 
              "dest": "/var/log/glance/readme.txt"
            }, 
            "name": "glance logs readme"
          }, 
          {
            "vars": {
              "netapp_nfs_backend_enable": false
            }, 
            "when": "netapp_nfs_backend_enable", 
            "name": "Mount Netapp NFS", 
            "block": [
              {
                "name": null, 
                "set_fact": {
                  "remote_file_path": "/etc/glance/glance-metadata-file.conf"
                }
              }, 
              {
                "name": null, 
                "file": {
                  "path": "{{ remote_file_path }}", 
                  "state": "touch"
                }
              }, 
              {
                "stat": "path=\"{{ remote_file_path }}\"", 
                "register": "file_path"
              }, 
              {
                "copy": {
                  "content": {
                    "mount_point": "/var/lib/glance/images", 
                    "type": "nfs", 
                    "share_location": "{{item.NETAPP_SHARE}}"
                  }, 
                  "dest": "{{ remote_file_path }}"
                }, 
                "when": [
                  "file_path.stat.exists == true"
                ], 
                "with_items": [
                  {
                    "NETAPP_SHARE": ""
                  }
                ]
              }, 
              {
                "mount": "name=/var/lib/glance/images src=\"{{item.NETAPP_SHARE}}\" fstype=nfs4 opts=\"{{item.NFS_OPTIONS}}\" state=mounted", 
                "name": null, 
                "with_items": [
                  {
                    "NETAPP_SHARE": "", 
                    "NFS_OPTIONS": "_netdev,bg,intr,context=system_u:object_r:glance_var_lib_t:s0"
                  }
                ]
              }
            ]
          }, 
          {
            "mount": "name=/var/lib/glance/images src=\"{{item.NFS_SHARE}}\" fstype=nfs4 opts=\"{{item.NFS_OPTIONS}}\" state=mounted", 
            "when": [
              "nfs_backend_enable"
            ], 
            "name": "Mount NFS on host", 
            "vars": {
              "nfs_backend_enable": false
            }, 
            "with_items": [
              {
                "NFS_OPTIONS": "_netdev,bg,intr,context=system_u:object_r:glance_var_lib_t:s0", 
                "NFS_SHARE": ""
              }
            ]
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/gnocchi", 
              "/var/log/containers/httpd/gnocchi-api"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from gnocchi containers can be found under\n/var/log/containers/gnocchi and /var/log/containers/httpd/gnocchi-api.\n", 
              "dest": "/var/log/gnocchi/readme.txt"
            }, 
            "name": "gnocchi logs readme"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "/var/log/containers/gnocchi", 
              "state": "directory"
            }
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/heat", 
              "/var/log/containers/httpd/heat-api"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from heat containers can be found under\n/var/log/containers/heat and /var/log/containers/httpd/heat-api*.\n", 
              "dest": "/var/log/heat/readme.txt"
            }, 
            "name": "heat logs readme"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/heat", 
              "/var/log/containers/httpd/heat-api-cfn"
            ]
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "/var/log/containers/heat", 
              "state": "directory"
            }
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/horizon", 
              "/var/log/containers/httpd/horizon"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from horizon containers can be found under\n/var/log/containers/horizon and /var/log/containers/httpd/horizon.\n", 
              "dest": "/var/log/horizon/readme.txt"
            }, 
            "name": "horizon logs readme"
          }, 
          {
            "stat": "path=/lib/systemd/system/iscsid.socket", 
            "register": "stat_iscsid_socket", 
            "name": "stat /lib/systemd/system/iscsid.socket"
          }, 
          {
            "when": "stat_iscsid_socket.stat.exists", 
            "name": "Stop and disable iscsid.socket service", 
            "service": "name=iscsid.socket state=stopped enabled=no"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/keystone", 
              "/var/log/containers/httpd/keystone"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from keystone containers can be found under\n/var/log/containers/keystone and /var/log/containers/httpd/keystone.\n", 
              "dest": "/var/log/keystone/readme.txt"
            }, 
            "name": "keystone logs readme"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "/var/log/containers/memcached", 
              "state": "directory"
            }
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from memcached containers can be found under\n/var/log/containers/memcached.\n", 
              "dest": "/var/log/memcached-readme.txt"
            }, 
            "name": "memcached logs readme"
          }, 
          {
            "name": "create /var/lib/mysql", 
            "file": {
              "path": "/var/lib/mysql", 
              "state": "directory"
            }
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/neutron", 
              "/var/log/containers/httpd/neutron-api"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from neutron containers can be found under\n/var/log/containers/neutron and /var/log/containers/httpd/neutron-api.\n", 
              "dest": "/var/log/neutron/readme.txt"
            }, 
            "name": "neutron logs readme"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/neutron"
            ]
          }, 
          {
            "name": "create /var/lib/neutron", 
            "file": {
              "path": "/var/lib/neutron", 
              "state": "directory"
            }
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/nova", 
              "/var/log/containers/httpd/nova-api"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from nova containers can be found under\n/var/log/containers/nova and /var/log/containers/httpd/nova-*.\n", 
              "dest": "/var/log/nova/readme.txt"
            }, 
            "name": "nova logs readme"
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "/var/log/containers/nova", 
              "state": "directory"
            }
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/nova", 
              "/var/log/containers/httpd/nova-placement"
            ]
          }, 
          {
            "name": "create persistent logs directory", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/var/log/containers/panko", 
              "/var/log/containers/httpd/panko-api"
            ]
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from panko containers can be found under\n/var/log/containers/panko and /var/log/containers/httpd/panko-api.\n", 
              "dest": "/var/log/panko/readme.txt"
            }, 
            "name": "panko logs readme"
          }, 
          {
            "name": "create /var/lib/rabbitmq", 
            "file": {
              "path": "/var/lib/rabbitmq", 
              "state": "directory"
            }
          }, 
          {
            "shell": "echo 'export ERL_EPMD_ADDRESS=127.0.0.1' > /etc/rabbitmq/rabbitmq-env.conf\necho 'export ERL_EPMD_PORT=4370' >> /etc/rabbitmq/rabbitmq-env.conf\nfor pid in $(pgrep epmd --ns 1 --nslist pid); do kill $pid; done\n", 
            "name": "stop the Erlang port mapper on the host and make sure it cannot bind to the port used by container"
          }, 
          {
            "name": "create /var/run/redis", 
            "file": {
              "path": "/var/run/redis", 
              "state": "directory"
            }
          }, 
          {
            "name": "create /var/log/redis", 
            "file": {
              "path": "/var/log/redis", 
              "state": "directory"
            }
          }, 
          {
            "name": "create /var/lib/redis", 
            "file": {
              "path": "/var/lib/redis", 
              "state": "directory"
            }
          }, 
          {
            "name": "create persistent directories", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/srv/node", 
              "/var/log/swift"
            ]
          }, 
          {
            "name": "Create swift logging symlink", 
            "file": {
              "dest": "/var/log/containers/swift", 
              "src": "/var/log/swift", 
              "state": "link"
            }
          }, 
          {
            "name": "create persistent directories", 
            "file": {
              "path": "{{ item }}", 
              "state": "directory"
            }, 
            "with_items": [
              "/srv/node", 
              "/var/log/swift", 
              "/var/log/containers"
            ]
          }, 
          {
            "name": "Set swift_use_local_disks fact", 
            "set_fact": {
              "swift_use_local_disks": true
            }
          }, 
          {
            "when": "swift_use_local_disks", 
            "name": "Create Swift d1 directory if needed", 
            "file": {
              "path": "/srv/node/d1", 
              "state": "directory"
            }
          }, 
          {
            "ignore_errors": true, 
            "copy": {
              "content": "Log files from swift containers can be found under\n/var/log/containers/swift and /var/log/containers/httpd/swift-*.\n", 
              "dest": "/var/log/swift/readme.txt"
            }, 
            "name": "swift logs readme"
          }, 
          {
            "with_items": [
              []
            ], 
            "name": "Format SwiftRawDisks", 
            "filesystem": {
              "opts": "-f -i size=1024", 
              "dev": "/dev/{{ item }}", 
              "fstype": "xfs"
            }
          }, 
          {
            "mount": {
              "src": "/dev/{{ item }}", 
              "state": "mounted", 
              "name": "/srv/node/{{ item }}", 
              "opts": "noatime", 
              "fstype": "xfs"
            }, 
            "name": "Mount devices defined in SwiftRawDisks", 
            "with_items": [
              []
            ]
          }, 
          {
            "name": "Create /var/lib/docker-puppet", 
            "file": "path=/var/lib/docker-puppet state=directory setype=svirt_sandbox_file_t selevel=s0 recurse=true"
          }, 
          {
            "copy": "content=\"{{docker_puppet_script}}\" dest=/var/lib/docker-puppet/docker-puppet.py force=yes mode=0600", 
            "name": "Write docker-puppet.py"
          }
        ], 
        "hosts": "localhost", 
        "vars": {
          "docker_puppet_script": "#!/usr/bin/env python\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n# Shell script tool to run puppet inside of the given docker container image.\n# Uses the config file at /var/lib/docker-puppet/docker-puppet.json as a source for a JSON\n# array of [config_volume, puppet_tags, manifest, config_image, [volumes]] settings\n# that can be used to generate config files or run ad-hoc puppet modules\n# inside of a container.\n\nimport glob\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\nimport tempfile\nimport time\nimport multiprocessing\n\nlogger = None\n\n\ndef get_logger():\n    global logger\n    if logger is None:\n        logger = logging.getLogger()\n        ch = logging.StreamHandler(sys.stdout)\n        if os.environ.get('DEBUG', False):\n            logger.setLevel(logging.DEBUG)\n            ch.setLevel(logging.DEBUG)\n        else:\n            logger.setLevel(logging.INFO)\n            ch.setLevel(logging.INFO)\n        formatter = logging.Formatter('%(asctime)s %(levelname)s: '\n                                      '%(process)s -- %(message)s')\n        ch.setFormatter(formatter)\n        logger.addHandler(ch)\n    return logger\n\n\n# this is to match what we do in deployed-server\ndef short_hostname():\n    subproc = subprocess.Popen(['hostname', '-s'],\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE)\n    cmd_stdout, cmd_stderr = subproc.communicate()\n    return cmd_stdout.rstrip()\n\n\ndef pull_image(name):\n    log.info('Pulling image: %s' % name)\n    retval = -1\n    count = 0\n    while retval != 0:\n        count += 1\n        subproc = subprocess.Popen(['/usr/bin/docker', 'pull', name],\n                                   stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE)\n\n        cmd_stdout, cmd_stderr = subproc.communicate()\n        retval = subproc.returncode\n        if retval != 0:\n            time.sleep(3)\n            log.warning('docker pull failed: %s' % cmd_stderr)\n            log.warning('retrying pulling image: %s' % name)\n        if count >= 5:\n            log.error('Failed to pull image: %s' % name)\n            break\n    if cmd_stdout:\n        log.debug(cmd_stdout)\n    if cmd_stderr:\n        log.debug(cmd_stderr)\n\n\ndef match_config_volumes(prefix, config):\n    # Match the mounted config volumes - we can't just use the\n    # key as e.g \"novacomute\" consumes config-data/nova\n    volumes = config.get('volumes', [])\n    return sorted([os.path.dirname(v.split(\":\")[0]) for v in volumes if\n                   v.startswith(prefix)])\n\n\ndef get_config_hash(config_volume):\n    hashfile = \"%s.md5sum\" % config_volume\n    log.debug(\"Looking for hashfile %s for config_volume %s\" % (hashfile, config_volume))\n    hash_data = None\n    if os.path.isfile(hashfile):\n        log.debug(\"Got hashfile %s for config_volume %s\" % (hashfile, config_volume))\n        with open(hashfile) as f:\n            hash_data = f.read().rstrip()\n    return hash_data\n\n\ndef rm_container(name):\n    if os.environ.get('SHOW_DIFF', None):\n        log.info('Diffing container: %s' % name)\n        subproc = subprocess.Popen(['/usr/bin/docker', 'diff', name],\n                                   stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE)\n        cmd_stdout, cmd_stderr = subproc.communicate()\n        if cmd_stdout:\n            log.debug(cmd_stdout)\n        if cmd_stderr:\n            log.debug(cmd_stderr)\n\n    log.info('Removing container: %s' % name)\n    subproc = subprocess.Popen(['/usr/bin/docker', 'rm', name],\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE)\n    cmd_stdout, cmd_stderr = subproc.communicate()\n    if cmd_stdout:\n        log.debug(cmd_stdout)\n    if cmd_stderr and \\\n           cmd_stderr != 'Error response from daemon: ' \\\n           'No such container: {}\\n'.format(name):\n        log.debug(cmd_stderr)\n\nprocess_count = int(os.environ.get('PROCESS_COUNT',\n                                   multiprocessing.cpu_count()))\nlog = get_logger()\nlog.info('Running docker-puppet')\nconfig_file = os.environ.get('CONFIG', '/var/lib/docker-puppet/docker-puppet.json')\n# If specified, only this config_volume will be used\nconfig_volume_only = os.environ.get('CONFIG_VOLUME', None)\nlog.debug('CONFIG: %s' % config_file)\nwith open(config_file) as f:\n    json_data = json.load(f)\n\n# To save time we support configuring 'shared' services at the same\n# time. For example configuring all of the heat services\n# in a single container pass makes sense and will save some time.\n# To support this we merge shared settings together here.\n#\n# We key off of config_volume as this should be the same for a\n# given group of services.  We are also now specifying the container\n# in which the services should be configured.  This should match\n# in all instances where the volume name is also the same.\n\nconfigs = {}\n\nfor service in (json_data or []):\n    if service is None:\n        continue\n    if isinstance(service, dict):\n        service = [\n            service.get('config_volume'),\n            service.get('puppet_tags'),\n            service.get('step_config'),\n            service.get('config_image'),\n            service.get('volumes', []),\n        ]\n\n    config_volume = service[0] or ''\n    puppet_tags = service[1] or ''\n    manifest = service[2] or ''\n    config_image = service[3] or ''\n    volumes = service[4] if len(service) > 4 else []\n\n    if not manifest or not config_image:\n        continue\n\n    log.debug('config_volume %s' % config_volume)\n    log.debug('puppet_tags %s' % puppet_tags)\n    log.debug('manifest %s' % manifest)\n    log.debug('config_image %s' % config_image)\n    log.debug('volumes %s' % volumes)\n    # We key off of config volume for all configs.\n    if config_volume in configs:\n        # Append puppet tags and manifest.\n        log.debug(\"Existing service, appending puppet tags and manifest\")\n        if puppet_tags:\n            configs[config_volume][1] = '%s,%s' % (configs[config_volume][1],\n                                                   puppet_tags)\n        if manifest:\n            configs[config_volume][2] = '%s\\n%s' % (configs[config_volume][2],\n                                                    manifest)\n        if configs[config_volume][3] != config_image:\n            log.warn(\"Config containers do not match even though\"\n                     \" shared volumes are the same!\")\n    else:\n        if not config_volume_only or (config_volume_only == config_volume):\n            log.debug(\"Adding new service\")\n            configs[config_volume] = service\n        else:\n            log.debug(\"Ignoring %s due to $CONFIG_VOLUME=%s\" %\n                (config_volume, config_volume_only))\n\nlog.info('Service compilation completed.')\n\n\ndef mp_puppet_config((config_volume, puppet_tags, manifest, config_image, volumes)):\n    log = get_logger()\n    log.info('Starting configuration of %s using image %s' % (config_volume,\n             config_image))\n    log.debug('config_volume %s' % config_volume)\n    log.debug('puppet_tags %s' % puppet_tags)\n    log.debug('manifest %s' % manifest)\n    log.debug('config_image %s' % config_image)\n    log.debug('volumes %s' % volumes)\n    sh_script = '/var/lib/docker-puppet/docker-puppet.sh'\n\n    with open(sh_script, 'w') as script_file:\n        os.chmod(script_file.name, 0755)\n        script_file.write(\"\"\"#!/bin/bash\n        set -ex\n        mkdir -p /etc/puppet\n        cp -a /tmp/puppet-etc/* /etc/puppet\n        rm -Rf /etc/puppet/ssl # not in use and causes permission errors\n        echo \"{\\\\\"step\\\\\": $STEP}\" > /etc/puppet/hieradata/docker.json\n        TAGS=\"\"\n        if [ -n \"$PUPPET_TAGS\" ]; then\n            TAGS=\"--tags \\\"$PUPPET_TAGS\\\"\"\n        fi\n\n        # Create a reference timestamp to easily find all files touched by\n        # puppet. The sync ensures we get all the files we want due to\n        # different timestamp.\n        origin_of_time=/var/lib/config-data/${NAME}.origin_of_time\n        touch $origin_of_time\n        sync\n\n        set +e\n        FACTER_hostname=$HOSTNAME FACTER_uuid=docker /usr/bin/puppet apply --summarize \\\n        --detailed-exitcodes --color=false --logdest syslog --logdest console --modulepath=/etc/puppet/modules:/usr/share/openstack-puppet/modules $TAGS /etc/config.pp\n        rc=$?\n        set -e\n        if [ $rc -ne 2 -a $rc -ne 0 ]; then\n            exit $rc\n        fi\n\n        # Disables archiving\n        if [ -z \"$NO_ARCHIVE\" ]; then\n            archivedirs=(\"/etc\" \"/root\" \"/opt\" \"/var/lib/ironic/tftpboot\" \"/var/lib/ironic/httpboot\" \"/var/www\" \"/var/spool/cron\" \"/var/lib/nova/.ssh\")\n            rsync_srcs=\"\"\n            for d in \"${archivedirs[@]}\"; do\n                if [ -d \"$d\" ]; then\n                    rsync_srcs+=\" $d\"\n                fi\n            done\n            rsync -a -R --delay-updates --delete-after $rsync_srcs /var/lib/config-data/${NAME}\n\n\n            # Also make a copy of files modified during puppet run\n            # This is useful for debugging\n            echo \"Gathering files modified after $(stat -c '%y' $origin_of_time)\"\n            mkdir -p /var/lib/config-data/puppet-generated/${NAME}\n            rsync -a -R -0 --delay-updates --delete-after \\\n                          --files-from=<(find $rsync_srcs -newer $origin_of_time -not -path '/etc/puppet*' -print0) \\\n                          / /var/lib/config-data/puppet-generated/${NAME}\n\n            # Write a checksum of the config-data dir, this is used as a\n            # salt to trigger container restart when the config changes\n            tar -c -f - /var/lib/config-data/${NAME} --mtime='1970-01-01' | md5sum | awk '{print $1}' > /var/lib/config-data/${NAME}.md5sum\n            tar -c -f - /var/lib/config-data/puppet-generated/${NAME} --mtime='1970-01-01' | md5sum | awk '{print $1}' > /var/lib/config-data/puppet-generated/${NAME}.md5sum\n        fi\n        \"\"\")\n\n    with tempfile.NamedTemporaryFile() as tmp_man:\n        with open(tmp_man.name, 'w') as man_file:\n            man_file.write('include ::tripleo::packages\\n')\n            man_file.write(manifest)\n\n        rm_container('docker-puppet-%s' % config_volume)\n        pull_image(config_image)\n\n        dcmd = ['/usr/bin/docker', 'run',\n                '--user', 'root',\n                '--name', 'docker-puppet-%s' % config_volume,\n                '--env', 'PUPPET_TAGS=%s' % puppet_tags,\n                '--env', 'NAME=%s' % config_volume,\n                '--env', 'HOSTNAME=%s' % short_hostname(),\n                '--env', 'NO_ARCHIVE=%s' % os.environ.get('NO_ARCHIVE', ''),\n                '--env', 'STEP=%s' % os.environ.get('STEP', '6'),\n                '--volume', '/etc/localtime:/etc/localtime:ro',\n                '--volume', '%s:/etc/config.pp:ro,z' % tmp_man.name,\n                '--volume', '/etc/puppet/:/tmp/puppet-etc/:ro,z',\n                '--volume', '/usr/share/openstack-puppet/modules/:/usr/share/openstack-puppet/modules/:ro,z',\n                '--volume', '%s:/var/lib/config-data/:z' % os.environ.get('CONFIG_VOLUME_PREFIX', '/var/lib/config-data'),\n                '--volume', 'tripleo_logs:/var/log/tripleo/',\n                # Syslog socket for puppet logs\n                '--volume', '/dev/log:/dev/log',\n                # OpenSSL trusted CA injection\n                '--volume', '/etc/pki/ca-trust/extracted:/etc/pki/ca-trust/extracted:ro',\n                '--volume', '/etc/pki/tls/certs/ca-bundle.crt:/etc/pki/tls/certs/ca-bundle.crt:ro',\n                '--volume', '/etc/pki/tls/certs/ca-bundle.trust.crt:/etc/pki/tls/certs/ca-bundle.trust.crt:ro',\n                '--volume', '/etc/pki/tls/cert.pem:/etc/pki/tls/cert.pem:ro',\n                # script injection\n                '--volume', '%s:%s:z' % (sh_script, sh_script) ]\n\n        for volume in volumes:\n            if volume:\n                dcmd.extend(['--volume', volume])\n\n        dcmd.extend(['--entrypoint', sh_script])\n\n        env = {}\n        # NOTE(flaper87): Always copy the DOCKER_* environment variables as\n        # they contain the access data for the docker daemon.\n        for k in filter(lambda k: k.startswith('DOCKER'), os.environ.keys()):\n            env[k] = os.environ.get(k)\n\n        if os.environ.get('NET_HOST', 'false') == 'true':\n            log.debug('NET_HOST enabled')\n            dcmd.extend(['--net', 'host', '--volume',\n                         '/etc/hosts:/etc/hosts:ro'])\n        dcmd.append(config_image)\n        log.debug('Running docker command: %s' % ' '.join(dcmd))\n\n        subproc = subprocess.Popen(dcmd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, env=env)\n        cmd_stdout, cmd_stderr = subproc.communicate()\n        # puppet with --detailed-exitcodes will return 0 for success and no changes\n        # and 2 for success and resource changes. Other numbers are failures\n        if subproc.returncode not in [0, 2]:\n            log.error('Failed running docker-puppet.py for %s' % config_volume)\n            if cmd_stdout:\n                log.error(cmd_stdout)\n            if cmd_stderr:\n                log.error(cmd_stderr)\n        else:\n            if cmd_stdout:\n                log.debug(cmd_stdout)\n            if cmd_stderr:\n                log.debug(cmd_stderr)\n            # only delete successful runs, for debugging\n            rm_container('docker-puppet-%s' % config_volume)\n\n        log.info('Finished processing puppet configs for %s' % (config_volume))\n        return subproc.returncode\n\n# Holds all the information for each process to consume.\n# Instead of starting them all linearly we run them using a process\n# pool.  This creates a list of arguments for the above function\n# to consume.\nprocess_map = []\n\nfor config_volume in configs:\n\n    service = configs[config_volume]\n    puppet_tags = service[1] or ''\n    manifest = service[2] or ''\n    config_image = service[3] or ''\n    volumes = service[4] if len(service) > 4 else []\n\n    if puppet_tags:\n        puppet_tags = \"file,file_line,concat,augeas,cron,%s\" % puppet_tags\n    else:\n        puppet_tags = \"file,file_line,concat,augeas,cron\"\n\n    process_map.append([config_volume, puppet_tags, manifest, config_image, volumes])\n\nfor p in process_map:\n    log.debug('- %s' % p)\n\n# Fire off processes to perform each configuration.  Defaults\n# to the number of CPUs on the system.\nlog.info('Starting multiprocess configuration steps.  Using %d processes.' %\n         process_count)\np = multiprocessing.Pool(process_count)\nreturncodes = list(p.map(mp_puppet_config, process_map))\nconfig_volumes = [pm[0] for pm in process_map]\nsuccess = True\nfor returncode, config_volume in zip(returncodes, config_volumes):\n    if returncode not in [0, 2]:\n        log.error('ERROR configuring %s' % config_volume)\n        success = False\n\n\n# Update the startup configs with the config hash we generated above\nconfig_volume_prefix = os.environ.get('CONFIG_VOLUME_PREFIX', '/var/lib/config-data')\nlog.debug('CONFIG_VOLUME_PREFIX: %s' % config_volume_prefix)\nstartup_configs = os.environ.get('STARTUP_CONFIG_PATTERN', '/var/lib/tripleo-config/docker-container-startup-config-step_*.json')\nlog.debug('STARTUP_CONFIG_PATTERN: %s' % startup_configs)\ninfiles = glob.glob('/var/lib/tripleo-config/docker-container-startup-config-step_*.json')\nfor infile in infiles:\n    with open(infile) as f:\n        infile_data = json.load(f)\n\n    for k, v in infile_data.iteritems():\n        config_volumes = match_config_volumes(config_volume_prefix, v)\n        config_hashes = [get_config_hash(volume_path) for volume_path in config_volumes]\n        config_hashes = filter(None, config_hashes)\n        config_hash = '-'.join(config_hashes)\n        if config_hash:\n            env = v.get('environment', [])\n            env.append(\"TRIPLEO_CONFIG_HASH=%s\" % config_hash)\n            log.debug(\"Updating config hash for %s, config_volume=%s hash=%s\" % (k, config_volume, config_hash))\n            infile_data[k]['environment'] = env\n\n    outfile = os.path.join(os.path.dirname(infile), \"hashed-\" + os.path.basename(infile))\n    with open(outfile, 'w') as out_f:\n        os.chmod(out_f.name, 0600)\n        json.dump(infile_data, out_f, indent=2)\n\nif not success:\n    sys.exit(1)\n", 
          "bootstrap_server_id": "6d15bb07-dd91-460e-ac11-e1b0a9a8abc6"
        }
      }
    ]
  creation_time: "2018-03-09T19:47:31Z"
  deployment_name: ControllerHostPrepDeployment
  group: ansible
  id: 8917b0e5-1ee5-4153-941d-3c8acd1327ea
  inputs:
    - name: deploy_server_id
      description: ID of the server being deployed to
      type: String
      value: |-
        083b33fa-9146-4fa6-8161-9fb5fdd0cc3f
    - name: deploy_action
      description: Name of the current action being deployed
      type: String
      value: |-
        CREATE
    - name: deploy_stack_id
      description: ID of the stack this deployment belongs to
      type: String
      value: |-
        overcloud-AllNodesDeploySteps-xmid3d3sdvjm-ControllerHostPrepDeployment-eysbpahv2wvj-1-ajhq25v2anop/a8103584-180f-4be0-bd25-18c17990d391
    - name: deploy_resource_name
      description: Name of this deployment resource in the stack
      type: String
      value: |-
        TripleOSoftwareDeployment
    - name: deploy_signal_transport
      description: How the server should signal to heat with the deployment output values.
      type: String
      value: |-
        NO_SIGNAL
  name: deployment_resource
  options: {u'modulepath': u'/usr/share/ansible-modules'}
  outputs:

